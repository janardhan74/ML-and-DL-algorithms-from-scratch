{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002946b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests , os , zipfile , shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ac5213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_url = \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
    "\n",
    "# zip_name = data_url.split(\"/\")[-1]\n",
    "# directory = zip_name.split(\".\")[0]\n",
    "\n",
    "# with open(zip_name , \"wb\") as f:\n",
    "#     r = requests.get(data_url)\n",
    "#     f.write(r.content)\n",
    "    \n",
    "# with zipfile.ZipFile(zip_name,\"r\") as zip_ref:\n",
    "#     zip_ref.extractall(directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8755af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"dataset\"\n",
    "# os.makedirs(data_dir,exist_ok=True)\n",
    "\n",
    "# shutil.move(r\"cornell_movie_dialogs_corpus\\cornell movie-dialogs corpus\\movie_conversations.txt\" , r\"dataset\\movie_conversations.txt\")\n",
    "# shutil.move(r\"cornell_movie_dialogs_corpus\\cornell movie-dialogs corpus\\movie_lines.txt\" , r\"dataset\\movie_lines.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d1c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32138523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d6dc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    seq_len = 64\n",
    "    batch_size = 128\n",
    "    num_heads = 4\n",
    "    embedding_dim = 128\n",
    "    \n",
    "    ffnn_units = 4*embedding_dim\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    nx = 2\n",
    "    num_heads = 4\n",
    "    \n",
    "    dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0627bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_movie_conv = \"./dataset/movie_conversations.txt\"\n",
    "corpus_movie_lines = \"./dataset/movie_lines.txt\"\n",
    "\n",
    "with open(corpus_movie_conv,\"r\",encoding=\"iso-8859-1\") as f:\n",
    "    conv = f.readlines()\n",
    "    \n",
    "with open(corpus_movie_lines,\"r\",encoding=\"iso-8859-1\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    \n",
    "lines_dic = {}\n",
    "for line in lines:\n",
    "    objects = line.split(\" +++$+++ \")\n",
    "    lines_dic[objects[0]] = objects[-1]\n",
    "    \n",
    "    \n",
    "pairs = []\n",
    "for con in conv:\n",
    "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs = []\n",
    "        \n",
    "        if i == len(ids)-1:\n",
    "            break\n",
    "        first = lines_dic[ids[i]].strip()\n",
    "        second = lines_dic[ids[i+1]].strip()\n",
    "        \n",
    "        qa_pairs.append(\" \".join(first.split()[:ModelArgs.seq_len]))\n",
    "        qa_pairs.append(\" \".join(second.split()[:ModelArgs.seq_len]))\n",
    "        \n",
    "        pairs.append(qa_pairs)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e39c0412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can we make this quick? Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad. Again.',\n",
       " \"Well, I thought we'd start with pronunciation, if that's okay with you.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11eb388a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./bert-it-l\\\\bert-it-vocab.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(\"./data\",exist_ok=True)\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in [x[0] for x in pairs]:\n",
    "    text_data.append(sample)\n",
    "    \n",
    "    if len(text_data)==1000:\n",
    "        with open(f\"./data/file_{file_count}.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "        \n",
    "paths = [str(x) for x in Path(\"./data\").glob('**/*.txt')]\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text = True,\n",
    "    handle_chinese_chars = False,\n",
    "    strip_accents = False,\n",
    "    lowercase = False\n",
    ")\n",
    "\n",
    "tokenizer.train(\n",
    "    files=paths,\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    limit_alphabet = 1000,\n",
    "    wordpieces_prefix=\"##\",\n",
    "    special_tokens=[\"[PAD]\",\"[CLS]\",\"[SEP]\",\"[MASK]\",\"[UNK]\"]\n",
    ")\n",
    "\n",
    "\n",
    "os.makedirs(\"./bert-it-l\",exist_ok=True)\n",
    "tokenizer.save_model(\"./bert-it-l\",\"bert-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73f42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(\"./bert-it-l/bert-it-vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3dcd5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self,data_pair, tokenizer, seq_len=ModelArgs.seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.corpus_lines = len(data_pair)\n",
    "        self.lines = data_pair\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        # 1. get a random sentence pair either it is positive or negative represend by is_next\n",
    "        t1,t2,is_next_label =self._get_sent(index)\n",
    "  \n",
    "  \n",
    "        # 2. replace random words in a sentece with mask / random wrods      \n",
    "        t1_random , t1_label = self._random_word(t1)\n",
    "        t2_random , t2_label = self._random_word(t2)\n",
    "        \n",
    "        # 3. Add cls and sep tokens at the start and end of the sequence & add pad token for the labels\n",
    "        t1 = [self.tokenizer.vocab[\"[CLS]\"]]+t1_random+[self.tokenizer.vocab[\"[SEP]\"]]\n",
    "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t1_label = [self.tokenizer.vocab[\"[PAD]\"]]+t1_label+[self.tokenizer.vocab[\"[PAD]\"]]\n",
    "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
    "        \n",
    "        # 4. combine sentence 1 and sentence 2\n",
    "        # add pad tokens to make the input sentence equal to the seq_len\n",
    "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
    "        bert_input = (t1+t2)[:self.seq_len]\n",
    "        bert_label = (t1_label+t2_label)[:self.seq_len]\n",
    "        padding = [self.tokenizer.vocab[\"[PAD]\"] for _ in range(self.seq_len - len(bert_input))]\n",
    "        segment_label.extend(padding)\n",
    "        bert_input.extend(padding)\n",
    "        bert_label.extend(padding)\n",
    "        \n",
    "        output = {\"bert_input\":bert_input,\n",
    "                  \"bert_label\":bert_label,\n",
    "                  \"segment_label\":segment_label,\n",
    "                  \"is_next_label\":is_next_label}\n",
    "        \n",
    "        return {key:torch.tensor(value) for key,value in output.items()}\n",
    "\n",
    "        \n",
    "    def _random_word(self,sentence):\n",
    "        tokens = sentence.split()\n",
    "        \n",
    "        output = []\n",
    "        output_label = []\n",
    "        \n",
    "        for i,token in enumerate(tokens):\n",
    "            prob = random.random()\n",
    "            \n",
    "            token_id = self.tokenizer(token)[\"input_ids\"][1:-1]\n",
    "            \n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "                \n",
    "                if prob < 0.8:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(self.tokenizer.vocab[\"[MASK]\"])\n",
    "                        \n",
    "                elif prob < 0.9:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(random.randrange(len(self.tokenizer.vocab)))\n",
    "            \n",
    "                else:\n",
    "                    output.append(token_id)\n",
    "                    \n",
    "                output_label.append(token_id)\n",
    "            else:\n",
    "                output.append(token_id)\n",
    "                for i in range(len(token_id)):\n",
    "                    output_label.append(0)\n",
    "                    \n",
    "        # flattening\n",
    "        output = list(itertools.chain(*[[x] if not isinstance(x , list) else x for x in output]))\n",
    "        output_label = list(itertools.chain(*[[x] if not isinstance(x,list) else x for x in output_label]))\n",
    "        \n",
    "        assert len(output) == len(output_label)\n",
    "        \n",
    "        return output,output_label\n",
    "    \n",
    "    \n",
    "    def _get_sent(self,index):\n",
    "        t1,t2 = self._get_corpus_line(index)\n",
    "        \n",
    "        p = random.random()\n",
    "        \n",
    "        if p > 0.5:\n",
    "            return t1,t2,1\n",
    "        else :\n",
    "            return t1,self._get_random_line(),0\n",
    "    \n",
    "    \n",
    "    def _get_corpus_line(self,index):\n",
    "        return self.lines[index][0],self.lines[index][1]\n",
    "        \n",
    "    def _get_random_line(self):\n",
    "        \"return a random sentence from the second pairs \"\n",
    "        return self.lines[random.randrange(len(self.lines))][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8dcf5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer(pairs[0][0])[\"input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cbb4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddings(nn.Module):\n",
    "    def __init__(self,embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pe = torch.zeros(size=(ModelArgs.seq_len,ModelArgs.embedding_dim))\n",
    "        \n",
    "        \n",
    "        for pos in range(self.pe.shape[0]):\n",
    "            for i in range(0,embedding_dim,2):\n",
    "                self.pe[pos,i] = math.sin(pos / 10_000 ** (2 * i / embedding_dim))\n",
    "                self.pe[pos,i+1] = math.cos(pos / 10_000 ** (2 * i / embedding_dim))\n",
    "            \n",
    "        self.register_buffer(\"pos_encoding\",self.pe)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self):\n",
    "        return self.pos_encoding.unsqueeze(0).expand(ModelArgs.batch_size , -1 , -1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "475e1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbeddings(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token = nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_dim)\n",
    "        self.segment = nn.Embedding(3,embedding_dim=embedding_dim)\n",
    "        self.position = PositionalEmbeddings(embedding_dim=embedding_dim)\n",
    "        self.dropout =  nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self,sequence,segment_labels):\n",
    "        \n",
    "        # print(self.token(sequence).device)\n",
    "        # print(self.segment(segment_labels).device)\n",
    "        # print(self.position().device)\n",
    "        embeddings = self.token(sequence) + self.segment(segment_labels) + self.position()\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dabb73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row = [0,0,0,0,1,1,1]\n",
    "# col = [\n",
    "#     0,\n",
    "#     0,\n",
    "#     0,\n",
    "#     1,\n",
    "#     1,\n",
    "#     1,\n",
    "#     1\n",
    "# ]\n",
    "\n",
    "\n",
    "# 0&0 0&0 0&0 0&0 1&0 1&0 1&0\n",
    "# 0&0 0&0 0&0 0&0 1&0 1&0 1&0\n",
    "# 0&0 0&0 0&0 0&0 1&0 1&0 1&0\n",
    "# 0&1 0&1 0&1 0&0 1&1 1&1 1&1\n",
    "# 0&1 0&1 0&1 0&1 1&1 1&1 1&1\n",
    "# 0&1 0&1 0&1 0&1 1&1 1&1 1&1\n",
    "# 0&1 0&1 0&1 0&1 1&1 1&1 1&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c0e37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,embedding_dim,num_heads ,dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert embedding_dim % num_heads == 0\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        self.query = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.key = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.value = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.out_proj = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)    \n",
    "        \n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        # query -> [batch_size , seq_len , embedding_dim]\n",
    "        # key -> [batch_size , seq_len , embedding_dim]\n",
    "        # value -> [batch_size , seq_len , embedding_dim]\n",
    "        \n",
    "        # mask -> [batch_size , seq_len]\n",
    "        \n",
    "        def shape(X):\n",
    "            return  X.view(ModelArgs.batch_size , self.num_heads , ModelArgs.seq_len , self.head_dim)\n",
    "        \n",
    "        query = shape(self.query(query)) # [batch_size , num_heads , seq_len , head_dim]\n",
    "        key = shape(self.key(key)) # [batch_size , num_heads , seq_len , head_dim]\n",
    "        value = shape(self.value(value)) # [batch_size , num_heads , seq_len , head_dim]\n",
    "        \n",
    "        scores = torch.matmul(query,value.transpose(-2,-1)) / (self.head_dim ** 0.5) # [batch_size , num_heads , seq_len(query) , seq_len(key)]\n",
    "        \n",
    "        if mask is not None:\n",
    "            q_mask = mask.unsqueeze(2) # [batch_size , seq_len , 1]\n",
    "            k_mask = mask.unsqueeze(1) # [batch_size , 1 , seq_len]\n",
    "            \n",
    "            full_mask = q_mask | k_mask \n",
    "            identiy = torch.eye(n=ModelArgs.seq_len , device=ModelArgs.device)\n",
    "            full_mask = full_mask + identiy # [batch_size , seq_len , seq_len ]\n",
    "            # print(full_mask.shape) # [batch_size , seq_len , seq_len ]\n",
    "            # print(scores.shape) #  [batch_size , num_heads , seq_len(query) , seq_len(key)]\n",
    "            full_mask = full_mask.unsqueeze(1).expand(-1,self.num_heads,-1,-1)\n",
    "            scores = scores.masked_fill(full_mask==1,float('-inf'))\n",
    "        \n",
    "        attn = torch.softmax(scores,dim=-1) # [batch_size , num_heads , seq_len , seq_le]\n",
    "        attn = self.dropout(attn) # [batch_size , num_heads , seq_len , seq_le]\n",
    "        \n",
    "        out = torch.matmul(attn,value) # [batch_size , num_heads , seq_len , seq_len] @ [batch_size , num_heads , seq_len , head_dim] = [batch_size , num_heads , seq_len , head_dim]\n",
    "        \n",
    "        out = out.transpose(-2,-1).contiguous().view(ModelArgs.batch_size,ModelArgs.seq_len,ModelArgs.embedding_dim)\n",
    "        \n",
    "        return self.out_proj(out) # use for how to adjust the context from different heads\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c84dd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self,embedding_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "    def forward(self,embeddings):\n",
    "        return self.norm(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea22a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddResidual(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,X1,X2):\n",
    "        return X1+X2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d5c7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self,embedding_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.fn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim,ModelArgs.ffnn_units),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ModelArgs.ffnn_units,embedding_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self,embeddings):\n",
    "        embeddings = self.fn(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4907827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,embedding_dim,num_heads,dropout):\n",
    "        super().__init__()\n",
    "        self.norm1 = Norm(embedding_dim=embedding_dim)\n",
    "        self.multi_head_attention = MultiHeadAttention(embedding_dim=embedding_dim,num_heads=num_heads,dropout=dropout)\n",
    "        self.add = AddResidual()\n",
    "        self.feed_forward_neural_network = FeedForwardNeuralNetwork(embedding_dim=embedding_dim,dropout=dropout)\n",
    "        self.norm2 = Norm(embedding_dim=embedding_dim)\n",
    "        \n",
    "    def forward(self,embeddings,mask):\n",
    "        # pre normalizatiom\n",
    "        \n",
    "        embeddings_norm = self.norm1(embeddings)\n",
    "        embeddings_norm_mha = self.multi_head_attention(query=embeddings_norm,key=embeddings_norm,value=embeddings_norm , mask=mask)\n",
    "        embeddings_norm_mha_add = self.add(embeddings_norm_mha,embeddings)\n",
    "        \n",
    "        embeddings_norm_mha_add_norm = self.norm2(embeddings_norm_mha_add)\n",
    "        embeddings_norm_mha_add_norm_fn = self.feed_forward_neural_network(embeddings_norm_mha_add_norm)\n",
    "        embeddings_norm_mha_add_norm_fn_add = self.add(embeddings_norm_mha_add,embeddings_norm_mha_add_norm_fn)\n",
    "        \n",
    "        return embeddings_norm_mha_add_norm_fn_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbac7f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self,nx,vocab_size,embedding_dim,num_heads,dropout):\n",
    "        super().__init__()\n",
    "        self.bert_embeddings = BERTEmbeddings(vocab_size=vocab_size,embedding_dim=embedding_dim,dropout=dropout)\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(embedding_dim=embedding_dim,num_heads=num_heads,dropout=dropout) for _ in range(nx)])\n",
    "        self.apply(self._init_weights)  \n",
    "        \n",
    "    def _init_weights(self,module):\n",
    "        if isinstance(module,nn.Linear):\n",
    "            nn.init.xavier_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module,nn.Embedding):\n",
    "            nn.init.xavier_normal_(module.weight)\n",
    "        elif isinstance(module,nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "        \n",
    "    def forward(self,sequence,segment_labels):\n",
    "        # sequnce -> [batch_size , seq_len ]\n",
    "        batch_size, seq_len = sequence.shape\n",
    "        mask = torch.zeros((batch_size, seq_len), device=sequence.device)\n",
    "\n",
    "        mask = (sequence == 0).long()  # Automatically sets 1 where padding token (id=0), else 0\n",
    "                \n",
    "        embeddings = self.bert_embeddings(sequence,segment_labels)\n",
    "        \n",
    "        for encoder in self.encoder_blocks:\n",
    "            embeddings = encoder(embeddings,mask)\n",
    "            \n",
    "        return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b115cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    2-class classification : is_next\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden,2)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        # print(\"NSP\")\n",
    "        # print(X.shape)\n",
    "        # X -> [batch_size , seq_len , embedding_dim]\n",
    "        # use the [CLS] toke for the classification head\n",
    "        X = self.linear(X[:,0,:])\n",
    "        return self.log_softmax(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3af96ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModel(nn.Module):\n",
    "    def __init__(self,hidden,vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden,vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    def forward(self,X):\n",
    "        X = self.linear(X)\n",
    "        return self.log_softmax(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5163a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLM(nn.Module):\n",
    "    def __init__(self,bert,embedding_dim,vocab_size):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.next_sentence = NextSentencePrediction(hidden=embedding_dim)\n",
    "        self.masked_lm = MaskedLanguageModel(hidden=embedding_dim,vocab_size=vocab_size)\n",
    "        \n",
    "    def forward(self,sequence,segment_label):\n",
    "        embeddings = self.bert(sequence,segment_label) # [batch_size , seq_len , embedding_dim]\n",
    "        \n",
    "        nsp = self.next_sentence(embeddings)\n",
    "        mlm = self.masked_lm(embeddings)\n",
    "        \n",
    "        return nsp,mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89d2b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchduledOptim():\n",
    "    \"\"\"A simple wrapper class for learning rate scheduling\"\"\"\n",
    "    def __init__(self,optimizer,d_model,warmup_steps , base_lrs =None):\n",
    "        self.optimizer = optimizer\n",
    "        self.init_lr = math.pow(d_model,-0.5)\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = 0\n",
    "        \n",
    "        if base_lrs is None:\n",
    "            self.base_lrs = [group['lr'] for group in self.optimizer.param_groups]\n",
    "        else:\n",
    "            self.base_lrs = base_lrs\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()    \n",
    "    \n",
    "    def step_and_update_lr(self):\n",
    "        self.optimizer.step()\n",
    "        self.update_lr()\n",
    "        \n",
    "    def update_lr(self):\n",
    "        self.current_step += 1\n",
    "        lr_scale =  self.get_lr_scale()\n",
    "        \n",
    "        # update the lr in optimizer\n",
    "        for base_lr , param_group in zip(self.base_lrs,self.optimizer.param_groups):\n",
    "            param_group['lr'] = base_lr * lr_scale\n",
    "        \n",
    "    def get_lr_scale(self):\n",
    "        return min(\n",
    "            math.pow(self.current_step,-0.5),\n",
    "            self.current_step*math.pow(self.warmup_steps,-1.5)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "913b246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class BERTTrainer():\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 embedding_dim,\n",
    "                 vocab_size,\n",
    "                 warmup_steps,\n",
    "                 train_dataloader,\n",
    "                 val_dataloader,\n",
    "                 epochs,\n",
    "                 lr,\n",
    "                 min_val_loss,\n",
    "                 betas,\n",
    "                 weight_decay,\n",
    "                 layer_decay,\n",
    "                 wandb,\n",
    "                 device):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.bert = model.to(device)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.min_val_loss = min_val_loss\n",
    "        \n",
    "        self.wandb = wandb\n",
    "        \n",
    "        param_groups = self.get_layerwise_lr_params(self.bert , base_lr=lr , layer_decay= layer_decay)\n",
    "        base_lrs = [group['lr'] for group in param_groups]\n",
    "        self._optimizer = torch.optim.Adam(param_groups,betas=betas,weight_decay=weight_decay,eps=1e-8)\n",
    "        self.scheduled_optimizer = SchduledOptim(optimizer=self._optimizer,d_model=embedding_dim,warmup_steps=warmup_steps,base_lrs=base_lrs)\n",
    "        self.criterion = nn.NLLLoss(ignore_index=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_layerwise_lr_params(self,model, base_lr, layer_decay=0.95):\n",
    "        \"\"\"\n",
    "        Returns param groups with exponentially decayed learning rates per layer.\n",
    "        \"\"\"\n",
    "        param_groups = []\n",
    "        assigned = set()\n",
    "\n",
    "        # Embeddings\n",
    "        param_groups.append({\n",
    "            \"params\": list(model.bert.bert_embeddings.parameters()),\n",
    "            \"lr\": base_lr * (layer_decay ** 0)\n",
    "        })\n",
    "        assigned |= set(model.bert.bert_embeddings.parameters())\n",
    "\n",
    "        # Encoder layers\n",
    "        for i, layer in enumerate(model.bert.encoder_blocks):\n",
    "            layer_params = list(layer.parameters())\n",
    "            param_groups.append({\n",
    "                \"params\": layer_params,\n",
    "                \"lr\": base_lr * (layer_decay ** (i + 1))  # deeper layers get smaller lr\n",
    "            })\n",
    "            assigned |= set(layer_params)\n",
    "\n",
    "        # Any remaining (top) layers like NSP or MLM head\n",
    "        remaining_params = [p for p in model.parameters() if p not in assigned]\n",
    "        if remaining_params:\n",
    "            param_groups.append({\n",
    "                \"params\": remaining_params,\n",
    "                \"lr\": base_lr  # full LR for heads\n",
    "            })\n",
    "\n",
    "        return param_groups\n",
    "\n",
    "    \n",
    "    def train_and_evaluate(self,log_grad_norm):\n",
    "        from tqdm import tqdm\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss,correct,total = 0.0,0,0\n",
    "            train_progress = tqdm(self.train_dataloader,desc=\"Training\")\n",
    "            for i,data in enumerate(train_progress):\n",
    "                data = {key:value.to(self.device) for key,value in data.items()}\n",
    "                \n",
    "                # bert_input : [batch_size , seq_len]\n",
    "                # bert_label : [batch_size , seq_len]\n",
    "                # segment_label : [batch_size , seq_len]\n",
    "                # is_next_label : a scalar value\n",
    "                \n",
    "                nsp_output , mlm_output = self.bert(data[\"bert_input\"],data[\"segment_label\"]) #  [batch_size , 2] ,  [batch_size , seq_len ,vocab_size ] \n",
    "                \n",
    "                nsp_loss = self.criterion(nsp_output,data[\"is_next_label\"])\n",
    "                \n",
    "                # mlm_output = [batch_size , seq_len , vocab_size] vs bert_label = [batch_size , seq_len] \n",
    "                mlm_output = mlm_output.view(-1,self.vocab_size)\n",
    "                data[\"bert_label\"] = data[\"bert_label\"].view(-1)\n",
    "                \n",
    "                mlm_loss = self.criterion(mlm_output,data['bert_label'])\n",
    "                loss = nsp_loss + mlm_loss\n",
    "                ###########################################################################################################################\n",
    "                self.scheduled_optimizer.zero_grad()\n",
    "                self._optimizer\n",
    "                loss.backward()\n",
    "                # --------------------------------------------------------------------------------------------------------------------------\n",
    "                torch.nn.utils.clip_grad_norm_(self.bert.parameters(),max_norm=1.0)\n",
    "                \n",
    "                if log_grad_norm:\n",
    "                    total_norm = 0\n",
    "                    for p in self.bert.parameters():\n",
    "                        if p.grad is not None:\n",
    "                            param_norm = p.grad.data.norm(2) # L2 norm\n",
    "                            total_norm = param_norm.item()**2\n",
    "                            \n",
    "                    \n",
    "\n",
    "                    total_norm = total_norm**0.5\n",
    "                    \n",
    "                    self.wandb.log({\"norm\":total_norm})\n",
    "\n",
    "                    grad_groups = defaultdict(list)\n",
    "\n",
    "                    for name, param in self.bert.named_parameters():\n",
    "                        if param.grad is None:\n",
    "                            continue\n",
    "                        # print(f\"name : {name} grad_norm : {param.grad.norm().item():.6f}\")\n",
    "                        # Get group name: e.g., \"encoder.0\", \"decoder.1\", \"embedding\", etc.\n",
    "                        tokens = name.split('.')\n",
    "                        group_name = '.'.join(tokens[:3]) if len(tokens) >= 3 else '.'.join(tokens[:2])\n",
    "                \n",
    "                        grad_norm = param.grad.norm().item()\n",
    "                        grad_groups[group_name].append(grad_norm)\n",
    "                \n",
    "                    # Average gradients per group\n",
    "                    avg_grad_per_group = {k: sum(v)/len(v) for k, v in grad_groups.items()}\n",
    "                    # ----------------------------------------------------------------------------------------------------------------------\n",
    "                    self.wandb.log(avg_grad_per_group)\n",
    "                    self.wandb.log({\"total_norm\":total_norm})\n",
    "                    \n",
    "                    self.scheduled_optimizer.step_and_update_lr()\n",
    "                ######################################################################################################################\n",
    "                \n",
    "                preds = torch.argmax(nsp_output,dim=-1)\n",
    "                correct += (preds == data[\"is_next_label\"]).sum().item()\n",
    "                total += ModelArgs.batch_size\n",
    "                \n",
    "                preds = torch.argmax(mlm_output,dim=-1) # [batch_size * seq_len]\n",
    "                correct += (preds == data[\"bert_label\"]).sum().item()\n",
    "                total += ModelArgs.batch_size * ModelArgs.seq_len\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                train_progress.set_postfix({\"nsp_loss\":f\"{nsp_loss.item():.4f}\",\"mlm_loss\":f\"{mlm_loss.item():.4f}\",\"loss\":f\"{loss.item():.4f}\"})\n",
    "                \n",
    "                self.wandb.log({\"train_loss\":train_loss/(i+1),\"train_acc\":correct/total})\n",
    "            \n",
    "            train_loss /= len(self.train_dataloader)\n",
    "            train_acc = correct / total\n",
    "                \n",
    "            # with torch.inference_mode():\n",
    "            #     val_loss,correct,total = 0.0,0,0\n",
    "            #     val_progress = tqdm(self.val_dataloader)\n",
    "            #     for i,data in enumerate(val_progress):\n",
    "            #         data = {key:value.to(self.device) for key,value in data.items()}\n",
    "                    \n",
    "            #         # bert_input : [batch_size , seq_len]\n",
    "            #         # bert_label : [batch_size , seq_len]\n",
    "            #         # segment_label : [batch_size , seq_len]\n",
    "            #         # is_next_label : a scalar value\n",
    "                    \n",
    "            #         nsp_output , mlm_output = self.bert(data[\"bert_input\"],data[\"segment_label\"])  #  [batch_size , 2] ,  [batch_size , seq_len ,vocab_size ] \n",
    "                    \n",
    "            #         nsp_loss = self.criterion(nsp_output,data[\"is_next_label\"])\n",
    "                    \n",
    "            #         mlm_loss = self.criterion(mlm_output,data[\"bert_label\"])\n",
    "                    \n",
    "            #         loss = nsp_loss + mlm_loss\n",
    "                    \n",
    "            #         preds = torch.argmax(nsp_output,dim=-1)\n",
    "            #         correct += (preds == data[\"is_next_label\"]).sum().item()\n",
    "            #         total += ModelArgs.batch_size\n",
    "                    \n",
    "            #         preds = torch.argmax(mlm_output,dim=-1).view(-1)\n",
    "            #         correct += (preds == data[\"bert_label\"].view(-1)).sum().item()\n",
    "            #         total += ModelArgs.batch_size*ModelArgs.seq_len\n",
    "                    \n",
    "            #         val_progress.set_postfix({\"nsp_loss\":f\"{nsp_loss.item():.4f}\",\"mlm_loss\":f\"{mlm_loss.item():.4f}\",\"loss\":f\"{loss.item():.4f}\"})\n",
    "                    \n",
    "            #         self.wandb.log({\"val_loss\":f\"{val_loss/i+1}\",\"val_acc\":f\"{correct/total}\"})\n",
    "                \n",
    "            #     val_loss /= len(self.val_dataloader)\n",
    "            #     val_acc = correct / total\n",
    "                \n",
    "                \n",
    "            print(f\"Epoch : {epoch+1}/{self.epochs} \\n train_loss : {train_loss:.5f} train_acc : {train_acc:.5f} \\n\")\n",
    "                \n",
    "            if train_loss < best_val_loss:\n",
    "                best_val_loss = train_loss\n",
    "                torch.save(self.bert.state_dict(),f\"bert_scratch_{train_loss:.4f}.pth\")\n",
    "            \n",
    "            if train_loss < self.min_val_loss:\n",
    "                print(\"[SUCCESS] model trained successfully\")\n",
    "                break\n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                 \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2aa26bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BERTDataset(data_pair=pairs,tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=ModelArgs.batch_size,\n",
    "                              shuffle=True,\n",
    "                              drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b1d61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af9b906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "431a44fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key : bert_input value : torch.Size([128, 64])\n",
      "key : bert_label value : torch.Size([128, 64])\n",
      "key : segment_label value : torch.Size([128, 64])\n",
      "key : is_next_label value : torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for key,value in sample.items():\n",
    "    print(f\"key : {key} value : {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f8ffd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BERT(nx=ModelArgs.nx , vocab_size=len(tokenizer.vocab) , embedding_dim=ModelArgs.embedding_dim , num_heads=ModelArgs.num_heads , dropout= ModelArgs.dropout) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43e79af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = bert.to(ModelArgs.device)\n",
    "bert(sample[\"bert_input\"].to(ModelArgs.device),sample[\"segment_label\"].to(ModelArgs.device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "378efc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 2]), torch.Size([128, 64, 24864]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_lm = BERTLM(bert=bert,embedding_dim=ModelArgs.embedding_dim , vocab_size=len(tokenizer.vocab))\n",
    "bert_lm = bert_lm.to(ModelArgs.device)\n",
    "sample_out = bert_lm(sample[\"bert_input\"].to(ModelArgs.device),sample[\"segment_label\"].to(ModelArgs.device))\n",
    "sample_out[0].shape,sample_out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74a89fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================================================================================================\n",
       "Layer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable\n",
       "======================================================================================================================================================\n",
       "BERT (BERT)                                                            [128, 64]            [128, 64, 128]       --                   True\n",
       "├─BERTEmbeddings (bert_embeddings)                                     [128, 64]            [128, 64, 128]       --                   True\n",
       "│    └─Embedding (token)                                               [128, 64]            [128, 64, 128]       3,182,592            True\n",
       "│    └─Embedding (segment)                                             [128, 64]            [128, 64, 128]       384                  True\n",
       "│    └─PositionalEmbeddings (position)                                 --                   [128, 64, 128]       --                   --\n",
       "├─ModuleList (encoder_blocks)                                          --                   --                   --                   True\n",
       "│    └─EncoderBlock (0)                                                [128, 64, 128]       [128, 64, 128]       --                   True\n",
       "│    │    └─Norm (norm1)                                               [128, 64, 128]       [128, 64, 128]       256                  True\n",
       "│    │    └─MultiHeadAttention (multi_head_attention)                  --                   [128, 64, 128]       66,048               True\n",
       "│    │    └─AddResidual (add)                                          [128, 64, 128]       [128, 64, 128]       --                   --\n",
       "│    │    └─Norm (norm2)                                               [128, 64, 128]       [128, 64, 128]       256                  True\n",
       "│    │    └─FeedForwardNeuralNetwork (feed_forward_neural_network)     [128, 64, 128]       [128, 64, 128]       131,712              True\n",
       "│    │    └─AddResidual (add)                                          [128, 64, 128]       [128, 64, 128]       --                   --\n",
       "│    └─EncoderBlock (1)                                                [128, 64, 128]       [128, 64, 128]       --                   True\n",
       "│    │    └─Norm (norm1)                                               [128, 64, 128]       [128, 64, 128]       256                  True\n",
       "│    │    └─MultiHeadAttention (multi_head_attention)                  --                   [128, 64, 128]       66,048               True\n",
       "│    │    └─AddResidual (add)                                          [128, 64, 128]       [128, 64, 128]       --                   --\n",
       "│    │    └─Norm (norm2)                                               [128, 64, 128]       [128, 64, 128]       256                  True\n",
       "│    │    └─FeedForwardNeuralNetwork (feed_forward_neural_network)     [128, 64, 128]       [128, 64, 128]       131,712              True\n",
       "│    │    └─AddResidual (add)                                          [128, 64, 128]       [128, 64, 128]       --                   --\n",
       "======================================================================================================================================================\n",
       "Total params: 3,579,520\n",
       "Trainable params: 3,579,520\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 458.18\n",
       "======================================================================================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 201.33\n",
       "Params size (MB): 14.32\n",
       "Estimated Total Size (MB): 215.78\n",
       "======================================================================================================================================================"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=bert,\n",
    "        input_data=(sample[\"bert_input\"].to(ModelArgs.device),sample[\"segment_label\"].to(ModelArgs.device)),\n",
    "        col_names=[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"],\n",
    "        device=ModelArgs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "533c437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\janar\\_netrc\n",
      "wandb: Currently logged in as: janardhanthippabattini (janardhanthippabattini-rajiv-gandhi-university-of-knowle) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import wandb\n",
    "dotenv.load_dotenv()\n",
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22c763e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_lm.load_state_dict(torch.load(r\"bert_scratch_7.8702.pth\",weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0dff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3193057967.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[36], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    for\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b06c56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = f\"bert_scratch-nsp&mlm layer_wise_learning_rate_decay=True,weight_init=normal,lr=1e-4,nx={ModelArgs.nx},heads={ModelArgs.num_heads},d_model={ModelArgs.embedding_dim},dropout={ModelArgs.dropout},scheduler=reserch_paper,optimizer=adam,seq_len={ModelArgs.seq_len},batch_size={ModelArgs.batch_size},warmup_steps=4_000,clip_grad_norm=True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aff0d6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>bert.bert_embeddings.segment</td><td>▆▆▆▅▄▅▇▅▆▇▅▇▄▆▅▆█▅▆▂▅▅▇▃▇▅▆▅▇▁▆▆▄▅▇▆▄▅▄▇</td></tr><tr><td>bert.bert_embeddings.token</td><td>▅▆▁▇▆▆▄▅█▄▅▆█▃▄▆▃▇▂▄▃▅▇▅▆▂▃▃▂▇▃▆▄█▂▇▁▆▆▇</td></tr><tr><td>bert.encoder_blocks.0</td><td>▆▆▆▄▅▅▆▄▄▇▄▂▅▅▁▂▅▅▁▄▁▄▃▅▆▇▃█▅▄▇▆▆▃▂▃▆▃▄▅</td></tr><tr><td>bert.encoder_blocks.1</td><td>▇▅▆▇▅▂▅▇▇█▇█▆▅▆▆▇█▄▇▆▃▇▇▆▄█▁██▆▇▆▃▄▇▅▅▇▆</td></tr><tr><td>masked_lm.linear.bias</td><td>▃▃▅▂▅▃▃▃▁▁▃▁█▇▂▄▃▃▁▃▃▃▇▃▁▄▂▄▄▂▄▂▃▂▂▄▅▆▃▂</td></tr><tr><td>masked_lm.linear.weight</td><td>▃▄▃▂▃▆▃▂▅▂▅▄▄▂▅█▆▃▄▅▄█▄▁▄▅▄▆▁▂▄▂▆▇▃▃▅▄▅▄</td></tr><tr><td>next_sentence.linear.bias</td><td>▃▂▂▆▃▅█▃▄▇▇▆▅▁▃▆▄▆▄▆▃▆▇▃▃▅▃▅▇▃▅▇▃▂▃▃▃▇▇▄</td></tr><tr><td>next_sentence.linear.weight</td><td>█▆▄▄▇▇▇▇▅▄▄▅▆▂▁▇▆▅▅▅▅▆▆▆▇█▅█▄▇▆▅▃▃▄▃▇▅▄▆</td></tr><tr><td>norm</td><td>▃▃▂▃▂▃▂▂▃▃▅▃▃▂▁▇▅▂▃▄▄▅▅▄▄▂▁▄█▃▂▄▄▃▃▃▅▅▅▄</td></tr><tr><td>total_norm</td><td>▃▃▁▂▅▄▁▂▂▂▄▅▄▃▁▁▂▆▅▃▅▄▃▃▃▅█▂▁▅▃▅▂▃▂▄▃▃▃▂</td></tr><tr><td>train_acc</td><td>▃▄██▁▅▆▆▅███▇█▇▆▆▆▆▇▇▇▇▇█▆▆▆▆▅▅▄▄▄▄▄▅▄▅▄</td></tr><tr><td>train_loss</td><td>▃█▃▁▂▇▆▅▅▅▅▅▄▄▄▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▅▅▅▅▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>bert.bert_embeddings.segment</td><td>0.04294</td></tr><tr><td>bert.bert_embeddings.token</td><td>0.03797</td></tr><tr><td>bert.encoder_blocks.0</td><td>0.07035</td></tr><tr><td>bert.encoder_blocks.1</td><td>0.06405</td></tr><tr><td>masked_lm.linear.bias</td><td>0.03762</td></tr><tr><td>masked_lm.linear.weight</td><td>0.83514</td></tr><tr><td>next_sentence.linear.bias</td><td>0.00227</td></tr><tr><td>next_sentence.linear.weight</td><td>0.04674</td></tr><tr><td>norm</td><td>0.03762</td></tr><tr><td>total_norm</td><td>0.03762</td></tr><tr><td>train_acc</td><td>0.01452</td></tr><tr><td>train_loss</td><td>7.7911</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert_scratch-nsp&mlm layer_wise_learning_rate_decay=True,weight_init=normal,lr=1e-4,nx=2,heads=4,d_model=128,dropout=0.1,scheduler=reserch_paper,optimizer=adam,seq_len=64,batch_size=128,warmup_steps=4_000,clip_grad_norm=True</strong> at: <a href='https://wandb.ai/janardhanthippabattini-rajiv-gandhi-university-of-knowle/bert_scratch-nsp%26mlm/runs/kjyk3r9c' target=\"_blank\">https://wandb.ai/janardhanthippabattini-rajiv-gandhi-university-of-knowle/bert_scratch-nsp%26mlm/runs/kjyk3r9c</a><br> View project at: <a href='https://wandb.ai/janardhanthippabattini-rajiv-gandhi-university-of-knowle/bert_scratch-nsp%26mlm' target=\"_blank\">https://wandb.ai/janardhanthippabattini-rajiv-gandhi-university-of-knowle/bert_scratch-nsp%26mlm</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250627_084705-kjyk3r9c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\janar\\OneDrive\\Documents\\AI\\Projects\\ML-and-DL-algorithms-from-scratch\\DL\\BERT\\wandb\\run-20250627_085140-rajnz4f6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/janardhanthippabattini-rajiv-gandhi-university-of-knowle/bert_scratch-nsp%26mlm/runs/rajnz4f6' target=\"_blank\">bert_scratch-nsp&mlm layer_wise_learning_rate_decay=True,weight_init=normal,lr=1e-4,nx=2,heads=4,d_model=128,dropout=0.1,scheduler=reserch_paper,optimizer=adam,seq_len=64,batch_size=128,warmup_steps=4_000,clip_grad_norm=True</a></strong> to <a href='https://wandb.ai/janardhanthippabattini-rajiv-gandhi-university-of-knowle/bert_scratch-nsp%26mlm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/janardhanthippabattini-rajiv-gandhi-university-of-knowle/bert_scratch-nsp%26mlm' target=\"_blank\">https://wandb.ai/janardhanthippabattini-rajiv-gandhi-university-of-knowle/bert_scratch-nsp%26mlm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/janardhanthippabattini-rajiv-gandhi-university-of-knowle/bert_scratch-nsp%26mlm/runs/rajnz4f6' target=\"_blank\">https://wandb.ai/janardhanthippabattini-rajiv-gandhi-university-of-knowle/bert_scratch-nsp%26mlm/runs/rajnz4f6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/janardhanthippabattini-rajiv-gandhi-university-of-knowle/bert_scratch-nsp%26mlm/runs/rajnz4f6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1dd645b83a0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"bert_scratch-nsp&mlm\",\n",
    "    name=exp_name,\n",
    "    # reinit=True,\n",
    "    id=\"rajnz4f6\",\n",
    "    resume=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06f887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "079b271f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24234"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14*1731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2768cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BERTTrainer(model=bert_lm,\n",
    "            embedding_dim=ModelArgs.embedding_dim,\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            warmup_steps=4_000,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=None,\n",
    "            epochs=200,\n",
    "            lr=1e-4,\n",
    "            min_val_loss=1e-2,\n",
    "            betas=(0.9,0.999),\n",
    "            weight_decay=0.1,\n",
    "            layer_decay=0.95,\n",
    "            wandb=wandb,    \n",
    "            device=ModelArgs.device\n",
    "            )\n",
    "\n",
    "trainer.scheduled_optimizer.current_step = 24234\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9069cd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [14:37<00:00,  1.97it/s, nsp_loss=0.0026, mlm_loss=7.7095, loss=7.7120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/200 \n",
      " train_loss : 7.71548 train_acc : 0.01441 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [16:16<00:00,  1.77it/s, nsp_loss=0.0025, mlm_loss=7.3963, loss=7.3988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2/200 \n",
      " train_loss : 7.60933 train_acc : 0.01437 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:55<00:00,  2.91it/s, nsp_loss=0.0021, mlm_loss=7.4242, loss=7.4263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3/200 \n",
      " train_loss : 7.50741 train_acc : 0.01440 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [11:33<00:00,  2.50it/s, nsp_loss=0.0021, mlm_loss=7.4698, loss=7.4719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4/200 \n",
      " train_loss : 7.42131 train_acc : 0.01439 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:37<00:00,  2.99it/s, nsp_loss=0.0018, mlm_loss=7.3663, loss=7.3681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5/200 \n",
      " train_loss : 7.34176 train_acc : 0.01446 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:30<00:00,  3.03it/s, nsp_loss=0.0018, mlm_loss=7.3366, loss=7.3383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6/200 \n",
      " train_loss : 7.27010 train_acc : 0.01450 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:27<00:00,  3.05it/s, nsp_loss=0.0016, mlm_loss=7.1562, loss=7.1578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7/200 \n",
      " train_loss : 7.20579 train_acc : 0.01449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:24<00:00,  3.06it/s, nsp_loss=0.0014, mlm_loss=7.0495, loss=7.0509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8/200 \n",
      " train_loss : 7.14859 train_acc : 0.01457 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:17<00:00,  3.10it/s, nsp_loss=0.0014, mlm_loss=7.1209, loss=7.1224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9/200 \n",
      " train_loss : 7.09846 train_acc : 0.01455 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [20:53<00:00,  1.38it/s, nsp_loss=0.0013, mlm_loss=6.8906, loss=6.8919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10/200 \n",
      " train_loss : 7.04691 train_acc : 0.01451 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [24:59<00:00,  1.15it/s, nsp_loss=0.0014, mlm_loss=7.2688, loss=7.2702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 11/200 \n",
      " train_loss : 7.00420 train_acc : 0.01451 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:17<00:00,  3.11it/s, nsp_loss=0.0013, mlm_loss=6.7739, loss=6.7753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 12/200 \n",
      " train_loss : 6.96027 train_acc : 0.01456 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:22<00:00,  3.08it/s, nsp_loss=0.0013, mlm_loss=7.2829, loss=7.2841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 13/200 \n",
      " train_loss : 6.92983 train_acc : 0.01455 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:32<00:00,  3.02it/s, nsp_loss=0.0013, mlm_loss=6.7767, loss=6.7780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 14/200 \n",
      " train_loss : 6.89251 train_acc : 0.01454 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:27<00:00,  3.05it/s, nsp_loss=0.0013, mlm_loss=6.7933, loss=6.7946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 15/200 \n",
      " train_loss : 6.84945 train_acc : 0.01459 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:26<00:00,  3.05it/s, nsp_loss=0.0013, mlm_loss=6.5736, loss=6.5749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 16/200 \n",
      " train_loss : 6.82113 train_acc : 0.01455 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:19<00:00,  3.09it/s, nsp_loss=0.0012, mlm_loss=6.7428, loss=6.7440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 17/200 \n",
      " train_loss : 6.79331 train_acc : 0.01460 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [10:38<00:00,  2.71it/s, nsp_loss=0.0011, mlm_loss=6.8537, loss=6.8548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 18/200 \n",
      " train_loss : 6.76502 train_acc : 0.01461 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [11:26<00:00,  2.52it/s, nsp_loss=0.0011, mlm_loss=6.5662, loss=6.5673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 19/200 \n",
      " train_loss : 6.74143 train_acc : 0.01454 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [12:24<00:00,  2.33it/s, nsp_loss=0.0011, mlm_loss=6.8324, loss=6.8335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 20/200 \n",
      " train_loss : 6.72159 train_acc : 0.01461 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [1:30:01<00:00,  3.12s/it, nsp_loss=0.0012, mlm_loss=6.8174, loss=6.8186]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 21/200 \n",
      " train_loss : 6.70040 train_acc : 0.01459 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [11:33<00:00,  2.50it/s, nsp_loss=0.0011, mlm_loss=6.3831, loss=6.3842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 22/200 \n",
      " train_loss : 6.67245 train_acc : 0.01461 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [11:24<00:00,  2.53it/s, nsp_loss=0.0010, mlm_loss=6.7139, loss=6.7149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 23/200 \n",
      " train_loss : 6.65459 train_acc : 0.01459 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:36<00:00,  3.00it/s, nsp_loss=0.0011, mlm_loss=6.6103, loss=6.6114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 24/200 \n",
      " train_loss : 6.63974 train_acc : 0.01458 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [10:11<00:00,  2.83it/s, nsp_loss=0.0011, mlm_loss=6.6269, loss=6.6279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 25/200 \n",
      " train_loss : 6.62418 train_acc : 0.01456 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [10:59<00:00,  2.63it/s, nsp_loss=0.0011, mlm_loss=6.5891, loss=6.5901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 26/200 \n",
      " train_loss : 6.60721 train_acc : 0.01462 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [10:54<00:00,  2.64it/s, nsp_loss=0.0010, mlm_loss=6.5592, loss=6.5602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 27/200 \n",
      " train_loss : 6.58842 train_acc : 0.01462 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1731/1731 [09:35<00:00,  3.01it/s, nsp_loss=0.0010, mlm_loss=6.4998, loss=6.5008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 28/200 \n",
      " train_loss : 6.57692 train_acc : 0.01460 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 191/1731 [01:01<08:18,  3.09it/s, nsp_loss=0.0010, mlm_loss=6.6831, loss=6.6841]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 76\u001b[0m, in \u001b[0;36mBERTTrainer.train_and_evaluate\u001b[1;34m(self, log_grad_norm)\u001b[0m\n\u001b[0;32m     74\u001b[0m train_loss,correct,total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     75\u001b[0m train_progress \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader,desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_progress):\n\u001b[0;32m     77\u001b[0m     data \u001b[38;5;241m=\u001b[39m {key:value\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m key,value \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# bert_input : [batch_size , seq_len]\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# bert_label : [batch_size , seq_len]\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# segment_label : [batch_size , seq_len]\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# is_next_label : a scalar value\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[11], line 42\u001b[0m, in \u001b[0;36mBERTDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     35\u001b[0m bert_label\u001b[38;5;241m.\u001b[39mextend(padding)\n\u001b[0;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_input\u001b[39m\u001b[38;5;124m\"\u001b[39m:bert_input,\n\u001b[0;32m     38\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_label\u001b[39m\u001b[38;5;124m\"\u001b[39m:bert_label,\n\u001b[0;32m     39\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegment_label\u001b[39m\u001b[38;5;124m\"\u001b[39m:segment_label,\n\u001b[0;32m     40\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_next_label\u001b[39m\u001b[38;5;124m\"\u001b[39m:is_next_label}\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {key:torch\u001b[38;5;241m.\u001b[39mtensor(value) \u001b[38;5;28;01mfor\u001b[39;00m key,value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems()}\n",
      "Cell \u001b[1;32mIn[11], line 42\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     35\u001b[0m bert_label\u001b[38;5;241m.\u001b[39mextend(padding)\n\u001b[0;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_input\u001b[39m\u001b[38;5;124m\"\u001b[39m:bert_input,\n\u001b[0;32m     38\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_label\u001b[39m\u001b[38;5;124m\"\u001b[39m:bert_label,\n\u001b[0;32m     39\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegment_label\u001b[39m\u001b[38;5;124m\"\u001b[39m:segment_label,\n\u001b[0;32m     40\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_next_label\u001b[39m\u001b[38;5;124m\"\u001b[39m:is_next_label}\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {key:\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key,value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train_and_evaluate(log_grad_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510fbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-1): 2 x EncoderBlock(\n",
       "    (norm1): Norm(\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (multi_head_attention): MultiHeadAttention(\n",
       "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (add): AddResidual()\n",
       "    (feed_forward_neural_network): FeedForwardNeuralNetwork(\n",
       "      (fn): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (norm2): Norm(\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_lm.bert.encoder_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf44ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d202ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_lm.load_state_dict(torch.load(r\"bert_scratc_6.7553.pth\",weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc675d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/6925 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.030429\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.083727\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.125002\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.118298\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.224823\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.049710\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.007839\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.001113\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.273066\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.038069\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.230888\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.020719\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.021772\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.018853\n",
      "name : next_sentence.linear.bias grad_norm : 0.000706\n",
      "name : masked_lm.linear.weight grad_norm : 0.881820\n",
      "name : masked_lm.linear.bias grad_norm : 0.034266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/6925 [00:00<1:01:23,  1.88it/s, nsp_loss=0.0013, mlm_loss=6.2047, loss=6.2060]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.022721\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.056245\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.085627\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.082773\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.153552\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.033794\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.003766\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.000527\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.298155\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.041441\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.158388\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.014239\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.014868\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.017752\n",
      "name : next_sentence.linear.bias grad_norm : 0.000653\n",
      "name : masked_lm.linear.weight grad_norm : 0.916347\n",
      "name : masked_lm.linear.bias grad_norm : 0.035622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/6925 [00:00<34:59,  3.30it/s, nsp_loss=0.0011, mlm_loss=6.5978, loss=6.5989]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.033137\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.077929\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.122776\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.123603\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.223369\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.049089\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.005716\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.000796\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.266545\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.037362\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.229658\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.020474\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.020958\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.017332\n",
      "name : next_sentence.linear.bias grad_norm : 0.000639\n",
      "name : masked_lm.linear.weight grad_norm : 0.884658\n",
      "name : masked_lm.linear.bias grad_norm : 0.034292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 3/6925 [00:00<27:24,  4.21it/s, nsp_loss=0.0011, mlm_loss=6.1897, loss=6.1909]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.032030\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.069821\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.100095\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.100504\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.184089\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.040362\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.006272\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.000893\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.251646\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.035120\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.189194\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.016719\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.017092\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.020585\n",
      "name : next_sentence.linear.bias grad_norm : 0.000756\n",
      "name : masked_lm.linear.weight grad_norm : 0.914235\n",
      "name : masked_lm.linear.bias grad_norm : 0.035300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 4/6925 [00:01<24:27,  4.72it/s, nsp_loss=0.0011, mlm_loss=6.5918, loss=6.5929]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.028416\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.070623\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.103616\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.097777\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.184908\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.041019\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.007499\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.001051\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.232419\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.032724\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.190417\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.016673\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.017063\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.016380\n",
      "name : next_sentence.linear.bias grad_norm : 0.000604\n",
      "name : masked_lm.linear.weight grad_norm : 0.918990\n",
      "name : masked_lm.linear.bias grad_norm : 0.035326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 5/6925 [00:01<23:19,  4.94it/s, nsp_loss=0.0011, mlm_loss=6.2718, loss=6.2729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.021399\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.056173\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.086489\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.083854\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.154368\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.034645\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.004101\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.000584\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.235203\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.032698\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.158899\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.014721\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.015218\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.019115\n",
      "name : next_sentence.linear.bias grad_norm : 0.000718\n",
      "name : masked_lm.linear.weight grad_norm : 0.934376\n",
      "name : masked_lm.linear.bias grad_norm : 0.036272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 6/6925 [00:01<22:48,  5.06it/s, nsp_loss=0.0013, mlm_loss=6.6226, loss=6.6238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.021973\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.026167\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.042617\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.043398\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.077726\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.017268\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.002282\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.000334\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.155420\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.021215\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.080599\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.007096\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.007157\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.020573\n",
      "name : next_sentence.linear.bias grad_norm : 0.000757\n",
      "name : masked_lm.linear.weight grad_norm : 0.977628\n",
      "name : masked_lm.linear.bias grad_norm : 0.037468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 7/6925 [00:01<20:58,  5.50it/s, nsp_loss=0.0011, mlm_loss=6.9632, loss=6.9643]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.019383\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.046990\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.063360\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.060545\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.113151\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.025333\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.004385\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.000627\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.230732\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.031701\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.117428\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.010575\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.010504\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.017860\n",
      "name : next_sentence.linear.bias grad_norm : 0.000664\n",
      "name : masked_lm.linear.weight grad_norm : 0.952024\n",
      "name : masked_lm.linear.bias grad_norm : 0.036721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 8/6925 [00:01<20:25,  5.64it/s, nsp_loss=0.0012, mlm_loss=6.7498, loss=6.7510]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.031175\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.058702\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.090762\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.091570\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.166051\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.036517\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.004072\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.000563\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.284337\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.039460\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.171247\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.015018\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.014934\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.016094\n",
      "name : next_sentence.linear.bias grad_norm : 0.000592\n",
      "name : masked_lm.linear.weight grad_norm : 0.914540\n",
      "name : masked_lm.linear.bias grad_norm : 0.034842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 9/6925 [00:01<20:10,  5.71it/s, nsp_loss=0.0011, mlm_loss=6.3247, loss=6.3259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.035593\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.068615\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.105126\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.100022\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.188317\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.041547\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.005211\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.000731\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.265353\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.037010\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.194244\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.017267\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.017243\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.017246\n",
      "name : next_sentence.linear.bias grad_norm : 0.000647\n",
      "name : masked_lm.linear.weight grad_norm : 0.907799\n",
      "name : masked_lm.linear.bias grad_norm : 0.035311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 10/6925 [00:02<18:53,  6.10it/s, nsp_loss=0.0012, mlm_loss=6.3010, loss=6.3022]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.035445\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.078642\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.124909\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.124744\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.225841\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.049104\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.004988\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.000680\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.295815\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.040859\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.232476\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.020440\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.021014\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.019290\n",
      "name : next_sentence.linear.bias grad_norm : 0.000724\n",
      "name : masked_lm.linear.weight grad_norm : 0.873132\n",
      "name : masked_lm.linear.bias grad_norm : 0.033784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 11/6925 [00:02<19:27,  5.92it/s, nsp_loss=0.0013, mlm_loss=6.1867, loss=6.1880]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.014054\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.028199\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.044826\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.043808\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.079238\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.017666\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.002833\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.000400\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.210466\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.028944\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.083097\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.007734\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.007813\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.018675\n",
      "name : next_sentence.linear.bias grad_norm : 0.000697\n",
      "name : masked_lm.linear.weight grad_norm : 0.966732\n",
      "name : masked_lm.linear.bias grad_norm : 0.037407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 12/6925 [00:02<19:58,  5.77it/s, nsp_loss=0.0012, mlm_loss=7.0329, loss=7.0342]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.033373\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.079571\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.119537\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.118098\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.217935\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.048171\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.006710\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.000941\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.318542\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.044124\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.223990\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.020140\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.019909\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.018015\n",
      "name : next_sentence.linear.bias grad_norm : 0.000666\n",
      "name : masked_lm.linear.weight grad_norm : 0.870973\n",
      "name : masked_lm.linear.bias grad_norm : 0.033883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 13/6925 [00:02<20:06,  5.73it/s, nsp_loss=0.0012, mlm_loss=6.2223, loss=6.2234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.017140\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.040809\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.056294\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.056162\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.102562\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.022175\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.001310\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.000185\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.177125\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.024269\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.106861\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.009379\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.009158\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.014747\n",
      "name : next_sentence.linear.bias grad_norm : 0.000549\n",
      "name : masked_lm.linear.weight grad_norm : 0.967245\n",
      "name : masked_lm.linear.bias grad_norm : 0.037025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 14/6925 [00:02<19:12,  6.00it/s, nsp_loss=0.0012, mlm_loss=6.6142, loss=6.6154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : bert.bert_embeddings.token.weight grad_norm : 0.020117\n",
      "name : bert.bert_embeddings.segment.weight grad_norm : 0.057771\n",
      "name : bert.encoder_blocks.0.norm1.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm1.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.query.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.value.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.multi_head_attention.out_proj.bias grad_norm : 0.083242\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.feed_forward_neural_network.fn.2.bias grad_norm : 0.084235\n",
      "name : bert.encoder_blocks.0.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.0.norm2.norm.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm1.norm.weight grad_norm : 0.150923\n",
      "name : bert.encoder_blocks.1.norm1.norm.bias grad_norm : 0.033134\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.weight grad_norm : 0.003200\n",
      "name : bert.encoder_blocks.1.multi_head_attention.query.bias grad_norm : 0.000449\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.weight grad_norm : 0.247233\n",
      "name : bert.encoder_blocks.1.multi_head_attention.value.bias grad_norm : 0.034275\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.weight grad_norm : 0.155628\n",
      "name : bert.encoder_blocks.1.multi_head_attention.out_proj.bias grad_norm : 0.014010\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.0.bias grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.feed_forward_neural_network.fn.2.bias grad_norm : 0.014301\n",
      "name : bert.encoder_blocks.1.norm2.norm.weight grad_norm : 0.000000\n",
      "name : bert.encoder_blocks.1.norm2.norm.bias grad_norm : 0.000000\n",
      "name : next_sentence.linear.weight grad_norm : 0.019069\n",
      "name : next_sentence.linear.bias grad_norm : 0.000704\n",
      "name : masked_lm.linear.weight grad_norm : 0.932611\n",
      "name : masked_lm.linear.bias grad_norm : 0.036050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 15/6925 [00:02<22:31,  5.11it/s, nsp_loss=0.0011, mlm_loss=6.6223, loss=6.6234]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 16\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m BERTTrainer(model\u001b[38;5;241m=\u001b[39mbert_lm,\n\u001b[0;32m      2\u001b[0m             embedding_dim\u001b[38;5;241m=\u001b[39mModelArgs\u001b[38;5;241m.\u001b[39membedding_dim,\n\u001b[0;32m      3\u001b[0m             vocab_size\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m             device\u001b[38;5;241m=\u001b[39mModelArgs\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m     14\u001b[0m             )\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 36\u001b[0m, in \u001b[0;36mBERTTrainer.train_and_evaluate\u001b[1;34m(self, log_grad_norm)\u001b[0m\n\u001b[0;32m     34\u001b[0m train_loss,correct,total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     35\u001b[0m train_progress \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader,desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_progress):\n\u001b[0;32m     37\u001b[0m     data \u001b[38;5;241m=\u001b[39m {key:value\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m key,value \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# bert_input : [batch_size , seq_len]\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# bert_label : [batch_size , seq_len]\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# segment_label : [batch_size , seq_len]\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# is_next_label : a scalar value\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[11], line 42\u001b[0m, in \u001b[0;36mBERTDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     35\u001b[0m bert_label\u001b[38;5;241m.\u001b[39mextend(padding)\n\u001b[0;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_input\u001b[39m\u001b[38;5;124m\"\u001b[39m:bert_input,\n\u001b[0;32m     38\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_label\u001b[39m\u001b[38;5;124m\"\u001b[39m:bert_label,\n\u001b[0;32m     39\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegment_label\u001b[39m\u001b[38;5;124m\"\u001b[39m:segment_label,\n\u001b[0;32m     40\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_next_label\u001b[39m\u001b[38;5;124m\"\u001b[39m:is_next_label}\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {key:torch\u001b[38;5;241m.\u001b[39mtensor(value) \u001b[38;5;28;01mfor\u001b[39;00m key,value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems()}\n",
      "Cell \u001b[1;32mIn[11], line 42\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     35\u001b[0m bert_label\u001b[38;5;241m.\u001b[39mextend(padding)\n\u001b[0;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_input\u001b[39m\u001b[38;5;124m\"\u001b[39m:bert_input,\n\u001b[0;32m     38\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_label\u001b[39m\u001b[38;5;124m\"\u001b[39m:bert_label,\n\u001b[0;32m     39\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegment_label\u001b[39m\u001b[38;5;124m\"\u001b[39m:segment_label,\n\u001b[0;32m     40\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_next_label\u001b[39m\u001b[38;5;124m\"\u001b[39m:is_next_label}\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {key:\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key,value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = BERTTrainer(model=bert_lm,\n",
    "            embedding_dim=ModelArgs.embedding_dim,\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            warmup_steps=10_000,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=None,\n",
    "            epochs=200,\n",
    "            lr=1e-9,\n",
    "            min_val_loss=1e-2,\n",
    "            betas=(0.9,0.999),\n",
    "            weight_decay=0.1,\n",
    "            wandb=wandb,    \n",
    "            device=ModelArgs.device\n",
    "            )\n",
    "\n",
    "trainer.train_and_evaluate(log_grad_norm=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989af462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTLM(\n",
       "  (bert): BERT(\n",
       "    (bert_embeddings): BERTEmbeddings(\n",
       "      (token): Embedding(24864, 128)\n",
       "      (segment): Embedding(3, 128)\n",
       "      (position): PositionalEmbeddings()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-1): 2 x EncoderBlock(\n",
       "        (norm1): Norm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (add): AddResidual()\n",
       "        (feed_forward_neural_network): FeedForwardNeuralNetwork(\n",
       "          (fn): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): Norm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (next_sentence): NextSentencePrediction(\n",
       "    (linear): Linear(in_features=128, out_features=2, bias=True)\n",
       "    (log_softmax): LogSoftmax(dim=-1)\n",
       "  )\n",
       "  (masked_lm): MaskedLanguageModel(\n",
       "    (linear): Linear(in_features=128, out_features=24864, bias=True)\n",
       "    (log_softmax): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d001b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stable)",
   "language": "python",
   "name": "stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
