{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import requests\nimport torch\nimport gzip\nimport shutil\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:28:21.937612Z","iopub.execute_input":"2025-06-12T04:28:21.937864Z","iopub.status.idle":"2025-06-12T04:28:26.932435Z","shell.execute_reply.started":"2025-06-12T04:28:21.937843Z","shell.execute_reply":"2025-06-12T04:28:26.931840Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def set_seed(seed:int=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:28:26.933692Z","iopub.execute_input":"2025-06-12T04:28:26.934180Z","iopub.status.idle":"2025-06-12T04:28:26.938010Z","shell.execute_reply.started":"2025-06-12T04:28:26.934155Z","shell.execute_reply":"2025-06-12T04:28:26.937250Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"base_url = \"https://github.com/multi30k/dataset/raw/refs/heads/master/data/task1/raw/\"\n\ntrain_url = (\"train.de.gz\",\"train.en.gz\")\nval_url = (\"val.de.gz\",\"val.en.gz\",)\ntest_url = (\"test_2016_flickr.de.gz\",\"test_2016_flickr.en.gz\",)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:28:26.938829Z","iopub.execute_input":"2025-06-12T04:28:26.939444Z","iopub.status.idle":"2025-06-12T04:28:26.953002Z","shell.execute_reply.started":"2025-06-12T04:28:26.939421Z","shell.execute_reply":"2025-06-12T04:28:26.952294Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def download(file_path,url):\n    with open(file_path,\"wb\") as f:\n        r = requests.get(url)\n        f.write(r.content)\n\n    return file_path\n\ntrain_paths = [download(url,base_url+url) for url in train_url]\nval_paths = [download(url,base_url+url) for url in val_url]\ntest_paths = [download(url,base_url+url) for url in test_url]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:28:26.954707Z","iopub.execute_input":"2025-06-12T04:28:26.954932Z","iopub.status.idle":"2025-06-12T04:28:29.791302Z","shell.execute_reply.started":"2025-06-12T04:28:26.954906Z","shell.execute_reply":"2025-06-12T04:28:29.790605Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def extract(in_file,out_file):\n    with gzip.open(in_file,\"rb\") as f_in:\n        with open(out_file,\"wb\") as f_out:\n            shutil.copyfileobj(f_in,f_out)\n\n    return out_file\n\ntrain_paths = [extract(file,file[:-3]) for file in train_paths]\nval_paths = [extract(file,file[:-3]) for file in val_paths]\ntest_paths = [extract(file,file[:-3]) for file in test_paths]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:28:29.792166Z","iopub.execute_input":"2025-06-12T04:28:29.792434Z","iopub.status.idle":"2025-06-12T04:28:29.832412Z","shell.execute_reply.started":"2025-06-12T04:28:29.792412Z","shell.execute_reply":"2025-06-12T04:28:29.831468Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from dataclasses import dataclass\n@dataclass\nclass ModelArgs():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    batch_size = 32\n    seq_len = 32\n    no_of_hidden_units_gru = 32\n    embedding_dim = 16\n    h_t_size = 32\n    en_vocab_size = None\n    de_vocab_size = None\n\n    embedding_dim =16\n    no_of_layers=4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:28:29.833413Z","iopub.execute_input":"2025-06-12T04:28:29.833686Z","iopub.status.idle":"2025-06-12T04:28:29.893227Z","shell.execute_reply.started":"2025-06-12T04:28:29.833664Z","shell.execute_reply":"2025-06-12T04:28:29.892291Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!python -m spacy download en_core_web_sm\n!python -m spacy download de_core_news_sm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:28:29.894068Z","iopub.execute_input":"2025-06-12T04:28:29.894359Z","iopub.status.idle":"2025-06-12T04:28:49.715999Z","shell.execute_reply.started":"2025-06-12T04:28:29.894312Z","shell.execute_reply":"2025-06-12T04:28:49.715030Z"}},"outputs":[{"name":"stdout","text":"Collecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\nCollecting de-core-news-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: de-core-news-sm\nSuccessfully installed de-core-news-sm-3.8.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('de_core_news_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import spacy\nfrom collections import deque,Counter,defaultdict\nimport io\n\nde_tokenizer = spacy.load(\"de_core_news_sm\")\nen_tokenizer = spacy.load(\"en_core_web_sm\")\n\ndef tokenize(text,tokenizer):\n    tokens = tokenizer(text)\n    return [token.text.lower for token in tokens if not token.is_space]\n\ndef build_vocab(file_path,tokenizer,min_freq=1,special_tokens=[\"<bos>\",\"<unk>\",\"<pad>\",\"<eos>\"]):\n    counter = Counter()\n    with io.open(file_path,encoding=\"utf-8\") as f:\n        for string_ in f:\n            tokens = tokenize(string_,tokenizer)\n            counter.update(tokens)\n\n    print(\"Completed extracting tokens...\")\n    tokens = [tok for tok,freq in counter.items() if freq>=min_freq]\n    vocab = {tok:idx for idx,tok in enumerate(tokens+special_tokens)}\n\n    \n    unk_idx = vocab[\"<unk>\"]\n\n    vocab = defaultdict(lambda : unk_idx,vocab)\n\n    return vocab\n\nde_vocab = build_vocab(train_paths[0],de_tokenizer)\nen_vocab = build_vocab(train_paths[1],en_tokenizer)\n\nModelArgs.de_vocab_size = len(de_vocab)+1\nModelArgs.en_vocab_size = len(en_vocab)+1\n\nModelArgs.de_vocab_size,ModelArgs.en_vocab_size            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:28:49.717272Z","iopub.execute_input":"2025-06-12T04:28:49.717543Z","iopub.status.idle":"2025-06-12T04:34:18.443140Z","shell.execute_reply.started":"2025-06-12T04:28:49.717513Z","shell.execute_reply":"2025-06-12T04:34:18.442516Z"}},"outputs":[{"name":"stdout","text":"Completed extracting tokens...\nCompleted extracting tokens...\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(322646, 297118)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def data_process(file_paths):\n    raw_de_iter = iter(io.open(file_paths[0],encoding=\"utf-8\"))\n    raw_en_iter = iter(io.open(file_paths[1],encoding=\"utf-8\"))\n\n    en_bos_idx = en_vocab[\"<bos>\"]\n    en_eos_idx = en_vocab[\"<eos>\"]\n\n    data = []\n    \n    for raw_de,raw_en in zip(raw_de_iter,raw_en_iter):\n        de_tensor = torch.tensor([de_vocab[token] for token in tokenize(raw_de,de_tokenizer)])\n        en_tensor = torch.tensor([en_vocab[token] for token in tokenize(raw_en,en_tokenizer)])\n\n        en_tensor = torch.cat([torch.tensor([en_bos_idx]),en_tensor,torch.tensor([en_eos_idx])])\n\n        de_tensor = torch.flip(de_tensor,dims=[0])\n\n        data.append((de_tensor,en_tensor))\n\n    return data\n\ntrain_data = data_process(train_paths)\nval_data = data_process(val_paths)\ntest_data = data_process(test_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:34:18.443828Z","iopub.execute_input":"2025-06-12T04:34:18.444303Z","iopub.status.idle":"2025-06-12T04:40:15.247410Z","shell.execute_reply.started":"2025-06-12T04:34:18.444284Z","shell.execute_reply":"2025-06-12T04:40:15.246618Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from torch.utils.data import Dataset,DataLoader\nclass TranslationDataset(Dataset):\n    def __init__(self,data):\n        self.data = data\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self,idx):\n        return self.data[idx]\n\ntrain_dataset = TranslationDataset(train_data)\nval_dataset = TranslationDataset(val_data)\ntest_dataset = TranslationDataset(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:40:15.250022Z","iopub.execute_input":"2025-06-12T04:40:15.250251Z","iopub.status.idle":"2025-06-12T04:40:15.254874Z","shell.execute_reply.started":"2025-06-12T04:40:15.250235Z","shell.execute_reply":"2025-06-12T04:40:15.254193Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def collate_fn(batch,seq_len=ModelArgs.seq_len):\n\n    de_batch,en_batch = zip(*batch)\n    def pad_or_truncate(sequence,pad_value):\n        if len(sequence) > seq_len:\n            return sequence[:seq_len]\n        else:\n            pad_len = seq_len - len(sequence)\n            return torch.cat([sequence,torch.full(size=[pad_len],fill_value=pad_value,dtype=sequence.dtype)])\n    de_batch = [pad_or_truncate(sample,pad_value=de_vocab[\"<pad>\"]) for sample in de_batch]\n    en_batch = [pad_or_truncate(sample,pad_value=en_vocab[\"<pad>\"]) for sample in en_batch]\n\n    de_batch = torch.stack(de_batch)\n    en_batch = torch.stack(en_batch)\n\n    return de_batch,en_batch\n\ntrain_dataloader = DataLoader(dataset=train_dataset,\n                             batch_size=ModelArgs.batch_size,\n                             shuffle=True,\n                             collate_fn=collate_fn,\n                             drop_last=True)\nval_dataloader = DataLoader(dataset=val_dataset,\n                            batch_size=ModelArgs.batch_size,\n                            shuffle=True,\n                            collate_fn=collate_fn,\n                           drop_last=True)\ntest_dataloader = DataLoader(dataset=test_dataset,\n                            batch_size=ModelArgs.batch_size,\n                            shuffle=True,\n                             collate_fn=collate_fn,\n                            drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:48:52.337571Z","iopub.execute_input":"2025-06-12T04:48:52.338259Z","iopub.status.idle":"2025-06-12T04:48:52.344727Z","shell.execute_reply.started":"2025-06-12T04:48:52.338233Z","shell.execute_reply":"2025-06-12T04:48:52.343865Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nclass ResetGate(nn.Module):\n    def __init__(self,h_t_size,embedding_dim):\n        super().__init__()\n        self.sigma_nn = nn.Sequential(\n            nn.Linear(in_features=h_t_size+embedding_dim,out_features=h_t_size),\n            nn.Sigmoid()\n        )\n\n    def forward(self,X_t,h_t):\n        # print(\"Reset\")\n        # print(f\"X_t : {X_t.shape} h_t : {h_t.shape}\")\n        combined = torch.cat([h_t,X_t],dim=1)\n        # print(f\"combined : {combined.shape}\")\n        # print(self.sigma_nn)\n        r_t = self.sigma_nn(combined)\n\n        return r_t\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:40:15.285398Z","iopub.execute_input":"2025-06-12T04:40:15.285607Z","iopub.status.idle":"2025-06-12T04:40:15.302944Z","shell.execute_reply.started":"2025-06-12T04:40:15.285592Z","shell.execute_reply":"2025-06-12T04:40:15.302279Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class UpdateGate(nn.Module):\n    def __init__(self,h_t_size,embedding_dim):\n        super().__init__()\n        self.sigma_nn = nn.Sequential(\n            nn.Linear(in_features=h_t_size+embedding_dim,out_features=h_t_size),\n            nn.Sigmoid()\n        )\n    def forward(self,X_t,h_t):\n        # print(\"Update\")\n        # print(f\"X_t : {X_t.shape} h_t : {h_t.shape}\")\n        combined = torch.cat([h_t,X_t],dim=1)\n        # print(f\"combined : {combined.shape}\")\n        # print(f\"in features : {self.sigma_nn}\")\n        z_t = self.sigma_nn(combined)\n\n        return z_t","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:40:15.303544Z","iopub.execute_input":"2025-06-12T04:40:15.303698Z","iopub.status.idle":"2025-06-12T04:40:15.327440Z","shell.execute_reply.started":"2025-06-12T04:40:15.303686Z","shell.execute_reply":"2025-06-12T04:40:15.326768Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class GRUCell(nn.Module):\n    def __init__(self,h_t_size,embedding_dim):\n        super().__init__()\n        self.reset_gate = ResetGate(h_t_size=h_t_size,embedding_dim=embedding_dim)\n        self.update_gate = UpdateGate(h_t_size=h_t_size,embedding_dim=embedding_dim)\n        self.tanh_nn = nn.Sequential(\n            nn.Linear(in_features=h_t_size+embedding_dim,out_features=h_t_size),\n            nn.Tanh()\n        )\n    def forward(self,X_t,h_t):\n\n        combined = torch.cat([h_t,X_t],dim=1)\n        \n        r_t = self.reset_gate(X_t,h_t)\n\n        modulated_hidden_state = h_t * r_t\n\n        modulated_hidden_state_X_t = torch.cat([modulated_hidden_state,X_t],dim=1)\n\n        candidate_hidden_state = self.tanh_nn(modulated_hidden_state_X_t)\n\n        z_t = self.update_gate(X_t,h_t)\n\n        h_t_f = (1-z_t) * h_t + z_t * candidate_hidden_state\n\n        return h_t_f","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:40:15.328146Z","iopub.execute_input":"2025-06-12T04:40:15.328441Z","iopub.status.idle":"2025-06-12T04:40:15.344553Z","shell.execute_reply.started":"2025-06-12T04:40:15.328413Z","shell.execute_reply":"2025-06-12T04:40:15.343922Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class GRUModel(nn.Module):\n    def __init__(self,h_t_size,embedding_dim):\n        \"\"\"\n        it take input for a single time step and perform the GRU operation return the hidden state\n        \"\"\"\n        super().__init__()\n        \n        self.gru_cell = GRUCell(h_t_size=h_t_size,embedding_dim=embedding_dim)\n\n    def forward(self,X_i,h_t=None):\n        if h_t == None:\n            h_t = torch.zeros(size=[ModelArgs.batch_size,ModelArgs.h_t_size],device=ModelArgs.device)\n\n        h_t_f = self.gru_cell(X_i,h_t)\n\n        return h_t","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:40:15.345233Z","iopub.execute_input":"2025-06-12T04:40:15.345661Z","iopub.status.idle":"2025-06-12T04:40:15.361050Z","shell.execute_reply.started":"2025-06-12T04:40:15.345645Z","shell.execute_reply":"2025-06-12T04:40:15.360312Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class Embeddings(nn.Module):\n    def __init__(self,vocab_size,embedding_dim):\n        super().__init__()\n        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_dim)\n    def forward(self,X):\n        return self.embedding_layer(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:40:15.361705Z","iopub.execute_input":"2025-06-12T04:40:15.361917Z","iopub.status.idle":"2025-06-12T04:40:15.381402Z","shell.execute_reply.started":"2025-06-12T04:40:15.361878Z","shell.execute_reply":"2025-06-12T04:40:15.380702Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self,h_t_size,embedding_dim,no_of_layers,vocab_size):\n        super().__init__()\n        self.embedding_layer = Embeddings(vocab_size=vocab_size,embedding_dim=embedding_dim)\n        self.encoder = nn.ModuleList([GRUModel(h_t_size=h_t_size,embedding_dim=embedding_dim)])\n        for layer in range(no_of_layers-1):\n            self.encoder.append(GRUModel(h_t_size=h_t_size,embedding_dim=h_t_size))\n\n    def forward(self,X_t):\n        hidden_states = []\n        s_0 = None\n        for timestep in range(ModelArgs.seq_len):\n            X_t_i = X_t[:,timestep]\n            e_i = self.embedding_layer(X_t_i)\n            # print(f\"e_i : {e_i.shape}\")\n            for layer in range(len(self.encoder)):\n                if timestep == 0:\n                    h_t = self.encoder[layer](e_i)\n                else:\n                    h_t = self.encoder[layer](e_i,h_t)\n                e_i = h_t\n            hidden_states.append(h_t)\n            s_0 = h_t # for now assume that s_0 (s_i-1 for 1st timestep to calculate attention for decoder ) is the hidden state output of last layer of last dimension\n        return torch.stack(hidden_states) , s_0\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:40:15.382015Z","iopub.execute_input":"2025-06-12T04:40:15.382169Z","iopub.status.idle":"2025-06-12T04:40:15.399923Z","shell.execute_reply.started":"2025-06-12T04:40:15.382157Z","shell.execute_reply":"2025-06-12T04:40:15.399398Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self,h_t_size,embedding_dim,vocab_size,no_of_layers):\n        super().__init__()\n        self.embedding_layer = Embeddings(vocab_size=vocab_size,embedding_dim=embedding_dim)\n        self.decoder = nn.ModuleList([GRUModel(h_t_size=h_t_size,embedding_dim=embedding_dim)])\n        for i in range(no_of_layers-1):\n            self.decoder.append(GRUModel(h_t_size=h_t_size,embedding_dim=h_t_size))\n        self.attention = BhandanauAttention(h_t_size,attn_dim=h_t_size)\n        self.classification_head = nn.Linear(in_features=h_t_size,out_features=vocab_size)\n\n    def forward(self,s_0,h_t_all,X_t=None):\n        all_logits = []\n        for timestep in range(ModelArgs.seq_len):\n            if X_t is not None:\n                X_t_i = X_t[:,timestep]\n                e_i = self.embedding_layer(X_t_i)\n            else:\n                if timestep == 0:\n                    X_t_i = torch.full(size=[ModelArgs.batch_size],fill_value=en_vocab[\"<bos>\"],device=ModelArgs.device)\n                    e_i = self.embedding_layer(X_t_i)\n                else:\n                    # print(f\"logits : {logits_curr_timestep.shape} timestep : {timestep}\")\n                    preds = torch.softmax(logits_curr_timestep,dim=-1) # on vocab dimension\n                    # print(f\"preds : {preds.shape}\")\n                    preds = torch.argmax(preds,dim=1) \n                    # print(f\"preds 2: {preds.shape}\")\"\"\n                    \n                    e_i = self.embedding_layer(preds)\n                    \n            c_t = self.attention(s_0,h_t_all)\n            h_t = c_t # already we send h_t (encoder) s_i-1 (previous step decoder output) so no need to again send h_t to layer of decoder\n            for layer in range(len(self.decoder)):\n                h_t = self.decoder[layer](e_i,h_t)\n                e_i = h_t\n            logits_curr_timestep = self.classification_head(h_t)\n            all_logits.append(logits_curr_timestep)\n            \n        return torch.stack(all_logits)\n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:42:22.299830Z","iopub.execute_input":"2025-06-12T04:42:22.300612Z","iopub.status.idle":"2025-06-12T04:42:22.307907Z","shell.execute_reply.started":"2025-06-12T04:42:22.300588Z","shell.execute_reply":"2025-06-12T04:42:22.307193Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"e-tj = v_a.T * tanh( W_a * S_i-1 + U_a * h_j )","metadata":{}},{"cell_type":"code","source":"class BhandanauAttention(nn.Module):\n    def __init__(self,h_t_size,attn_dim):\n        super().__init__()\n        self.W_a = nn.Linear(in_features=h_t_size,out_features=attn_dim)\n        self.U_a = nn.Linear(in_features=h_t_size,out_features=attn_dim)\n        self.V_a = nn.Linear(in_features=attn_dim,out_features=1)\n\n    def forward(self,s,h):\n        # print(f\"h :{h.shape}\")\n        s_proj = self.W_a(s) \n        h_proj = self.U_a(h)\n        # print(f\"s_proj : {s_proj.shape}\")\n        # print(f\"h_proj : {h_proj.shape}\")\n        energy = torch.tanh(s_proj + h_proj)\n        # print(f\"energy : {energy.shape}\")\n        e_tj = self.V_a(energy)\n\n        # print(f\"e_tj : {e_tj.shape}\")\n        alpha_tj = torch.softmax(e_tj,dim=1).squeeze()\n        # print(f\"alpha_tj : {alpha_tj.shape}\")\n        context = torch.bmm(alpha_tj.unsqueeze(1),h).squeeze()\n        # print(f\"context : {context.shape}\")\n        return context","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:40:15.417882Z","iopub.execute_input":"2025-06-12T04:40:15.418142Z","iopub.status.idle":"2025-06-12T04:40:15.436378Z","shell.execute_reply.started":"2025-06-12T04:40:15.418120Z","shell.execute_reply":"2025-06-12T04:40:15.435807Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self,h_t_size,embedding_dim,src_vocab_size,dst_vocab_size,no_of_layers):\n        super().__init__()\n        self.encoder = Encoder(h_t_size=h_t_size,embedding_dim=embedding_dim,no_of_layers=no_of_layers,vocab_size=src_vocab_size)\n        self.decoder = Decoder(h_t_size=h_t_size,embedding_dim=embedding_dim,no_of_layers=no_of_layers,vocab_size=dst_vocab_size)\n        \n\n    def forward(self,X,y=None):\n        h_t_all,s_0 = self.encoder(X)\n        # generate c_t for every time step since s_i changes (previous time step output) \n        if y is not None:\n            outputs = self.decoder(s_0=s_0,h_t_all=h_t_all,X_t=y)\n        else:\n            outputs = self.decoder(s_0=s_0,h_t_all=h_t_all)\n            \n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:40:15.437119Z","iopub.execute_input":"2025-06-12T04:40:15.437391Z","iopub.status.idle":"2025-06-12T04:40:15.497256Z","shell.execute_reply.started":"2025-06-12T04:40:15.437371Z","shell.execute_reply":"2025-06-12T04:40:15.496588Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"sample_batch = next(iter(train_dataloader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:40:15.497815Z","iopub.execute_input":"2025-06-12T04:40:15.498048Z","iopub.status.idle":"2025-06-12T04:40:15.575528Z","shell.execute_reply.started":"2025-06-12T04:40:15.498033Z","shell.execute_reply":"2025-06-12T04:40:15.574854Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"sample_de = sample_batch[0]\nsample_en = sample_batch[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:40:15.576326Z","iopub.execute_input":"2025-06-12T04:40:15.576578Z","iopub.status.idle":"2025-06-12T04:40:15.579806Z","shell.execute_reply.started":"2025-06-12T04:40:15.576556Z","shell.execute_reply":"2025-06-12T04:40:15.579234Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"sample_de.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:40:15.580582Z","iopub.execute_input":"2025-06-12T04:40:15.580808Z","iopub.status.idle":"2025-06-12T04:40:15.597149Z","shell.execute_reply.started":"2025-06-12T04:40:15.580792Z","shell.execute_reply":"2025-06-12T04:40:15.596451Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 32])"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"sample_en.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:40:15.597913Z","iopub.execute_input":"2025-06-12T04:40:15.598150Z","iopub.status.idle":"2025-06-12T04:40:15.614982Z","shell.execute_reply.started":"2025-06-12T04:40:15.598129Z","shell.execute_reply":"2025-06-12T04:40:15.614440Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 32])"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"model = Seq2Seq(h_t_size=ModelArgs.h_t_size,\n                embedding_dim=ModelArgs.embedding_dim,\n                src_vocab_size=ModelArgs.de_vocab_size,\n                dst_vocab_size=ModelArgs.en_vocab_size,\n                no_of_layers=ModelArgs.no_of_layers)\nmodel = model.to(ModelArgs.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:42:26.701687Z","iopub.execute_input":"2025-06-12T04:42:26.702254Z","iopub.status.idle":"2025-06-12T04:42:26.887392Z","shell.execute_reply.started":"2025-06-12T04:42:26.702229Z","shell.execute_reply":"2025-06-12T04:42:26.886811Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"all_logits = model(sample_de.to(ModelArgs.device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:43:05.452713Z","iopub.execute_input":"2025-06-12T04:43:05.453354Z","iopub.status.idle":"2025-06-12T04:43:06.150533Z","shell.execute_reply.started":"2025-06-12T04:43:05.453321Z","shell.execute_reply":"2025-06-12T04:43:06.150001Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"all_logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:43:09.126031Z","iopub.execute_input":"2025-06-12T04:43:09.126516Z","iopub.status.idle":"2025-06-12T04:43:09.131446Z","shell.execute_reply.started":"2025-06-12T04:43:09.126493Z","shell.execute_reply":"2025-06-12T04:43:09.130687Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 32, 297118])"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"all_logits = model(sample_de.to(ModelArgs.device),sample_en.to(ModelArgs.device))\nall_logits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:43:29.334351Z","iopub.execute_input":"2025-06-12T04:43:29.334608Z","iopub.status.idle":"2025-06-12T04:43:29.473611Z","shell.execute_reply.started":"2025-06-12T04:43:29.334589Z","shell.execute_reply":"2025-06-12T04:43:29.473054Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 32, 297118])"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(model,model_name,criterion,optimizer,train_dataloader,val_dataloader,epochs,min_val_loss,device):\n    from tqdm import tqdm\n    model = model.to(device)\n    best_val_loss = float('inf')\n    for epoch in range(epochs):\n        train_loss,correct,total = 0.0,0,0\n        model.train()\n        train_progress = tqdm(train_dataloader,desc=\"Training\")\n        for de_batch,en_batch in train_progress:\n            de_batch = de_batch.to(device)\n            en_batch = en_batch.to(device)\n\n            optimizer.zero_grad()\n\n            all_logits = model(de_batch,en_batch)\n            all_logits = all_logits.view(-1,ModelArgs.en_vocab_size)\n            en_batch = en_batch.view(-1)\n\n            loss = criterion(all_logits,en_batch)\n            loss.backward()\n            optimizer.step()\n\n            preds = torch.softmax(all_logits,dim=-1)\n            preds = torch.argmax(preds,dim=-1)\n            \n            train_loss += loss.item()\n            correct += (preds == en_batch).sum()\n            total += en_batch.shape[0]\n\n            train_progress.set_postfix({\"loss\":f\"{loss.item():.2f}\"})\n        train_loss /= len(train_dataloader)\n        train_acc = correct/total\n        with torch.inference_mode():\n            val_loss,correct,total = 0.0,0,0\n            val_progress = tqdm(val_dataloader,desc=\"Evaluation\")\n            for de_batch,en_batch in val_progress:\n                de_batch = de_batch.to(device)\n                en_batch = en_batch.to(device)\n\n                all_logits = model(de_batch,en_batch)\n                all_logits = all_logits.view(-1,ModelArgs.en_vocab_size)\n                en_batch = en_batch.view(-1)\n                loss = criterion(all_logits,en_batch)\n\n                preds = torch.softmax(all_logits,dim=-1)\n                preds = torch.argmax(preds,dim=-1)\n\n                val_loss += loss.item()\n                correct += (preds==en_batch).sum()\n                total += en_batch.shape[0]\n\n            val_loss /= len(val_dataloader)\n            val_acc = correct/total\n\n        print(f\"Epoch : {epoch+1}/{epochs}\\n Train Loss : {train_loss:.5f} Train Acc : {train_acc:.4f} \\n Val Loss : {val_loss:.5f} Val Acc : {val_acc:.4f} \\n\\n\")\n        if best_val_loss > val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(),model_name)\n        if val_loss < min_val_loss:\n            print(\"Model trained successfully...\")\n            break\n        \n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T04:49:02.863156Z","iopub.execute_input":"2025-06-12T04:49:02.863417Z","iopub.status.idle":"2025-06-12T04:49:02.874544Z","shell.execute_reply.started":"2025-06-12T04:49:02.863400Z","shell.execute_reply":"2025-06-12T04:49:02.873945Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\ntrain(model=model,\n      model_name=\"Bhandanau_Attention_scrath.pth\",\n      criterion=criterion,\n     optimizer=optimizer,\n     train_dataloader=train_dataloader,\n     val_dataloader=val_dataloader,\n     epochs=500,\n     min_val_loss=1e-2,\n     device=ModelArgs.device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}