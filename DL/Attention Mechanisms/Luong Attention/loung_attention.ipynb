{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d3296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import shutil\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ed1cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in c:\\users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages (from tensorboard) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages (from tensorboard) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages (from tensorboard) (75.8.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db31e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs():\n",
    "    batch_size = 32\n",
    "    seq_len = 32\n",
    "    h_t_size = 32\n",
    "    c_t_size = 32\n",
    "    no_of_hidden_units_lstm = h_t_size\n",
    "    \n",
    "    max_lr = 1e-4\n",
    "    epochs = 5000\n",
    "    en_vocab_size = None\n",
    "    de_vocab_size = None\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a87485ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Attempt 2 failed: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Downloaded: train.de.gz\n",
      "Downloaded: train.en.gz\n",
      "Attempt 1 failed: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Downloaded: val.de.gz\n",
      "Downloaded: val.en.gz\n",
      "Attempt 1 failed: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Downloaded: test_2016_flickr.de.gz\n",
      "Downloaded: test_2016_flickr.en.gz\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://github.com/multi30k/dataset/raw/refs/heads/master/data/task1/raw/\"\n",
    "\n",
    "train_url = (\"train.de.gz\",\"train.en.gz\")\n",
    "val_url = (\"val.de.gz\",\"val.en.gz\",)\n",
    "test_url = (\"test_2016_flickr.de.gz\",\"test_2016_flickr.en.gz\",)\n",
    "\n",
    "from time import sleep\n",
    "def download_data(file_name, url, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=10) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(file_name, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "            print(f\"Downloaded: {file_name}\")\n",
    "            break\n",
    "        except (requests.exceptions.RequestException, ConnectionResetError) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            sleep(2)  # wait before retrying\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"Failed to download {file_name} after {retries} attempts.\")\n",
    "    return file_name\n",
    "\n",
    "train_paths = [download_data(path,base_url+path) for path in train_url]\n",
    "val_paths = [download_data(path,base_url+path) for path in val_url]\n",
    "test_paths = [download_data(path,base_url+path) for path in test_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "872c4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(in_file,out_file):\n",
    "    with gzip.open(in_file,\"rb\") as f_in:\n",
    "        with open(out_file,\"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in,f_out)\n",
    "    return out_file\n",
    "train_paths = [extract_data(file,file[:-3]) for file in train_paths]\n",
    "val_paths = [extract_data(file,file[:-3]) for file in val_paths]\n",
    "test_paths = [extract_data(file,file[:-3]) for file in test_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc07503",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vocab\n\u001b[0;32m     26\u001b[0m de_vocab \u001b[38;5;241m=\u001b[39m build_vocab(train_paths[\u001b[38;5;241m0\u001b[39m],tokenizer\u001b[38;5;241m=\u001b[39mde_tokenizer)\n\u001b[1;32m---> 27\u001b[0m en_vocab \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43men_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m ModelArgs\u001b[38;5;241m.\u001b[39men_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(en_vocab)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     30\u001b[0m ModelArgs\u001b[38;5;241m.\u001b[39mde_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(de_vocab)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mbuild_vocab\u001b[1;34m(file_name, tokenizer, min_freq, special_tokens)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(file_name,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m string_ \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m---> 16\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring_\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m         counter\u001b[38;5;241m.\u001b[39mupdate(tokens)\n\u001b[0;32m     19\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token,freq \u001b[38;5;129;01min\u001b[39;00m counter\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m freq \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_freq]\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(text, tokenizer)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize\u001b[39m(text,tokenizer):\n\u001b[1;32m----> 9\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [token\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_space]\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\spacy\\language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m proc(doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg\u001b[38;5;241m.\u001b[39mget(name, {}))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\spacy\\pipeline\\tok2vec.py:126\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m    124\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc((\u001b[38;5;241m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m--> 126\u001b[0m tokvecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\layers\\with_array.py:36\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     33\u001b[0m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m     34\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[SeqT, Callable]:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Ragged):\n\u001b[1;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_ragged_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Padded):\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\layers\\with_array.py:91\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[1;34m(model, Xr, is_train)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_ragged_forward\u001b[39m(\n\u001b[0;32m     88\u001b[0m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[0;32m     90\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 91\u001b[0m     Y, get_dX \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataXd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackprop\u001b[39m(dYr: Ragged) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Ragged:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[38;5;241m.\u001b[39mdataXd), dYr\u001b[38;5;241m.\u001b[39mlengths)\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\layers\\maxout.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m Y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mgemm(X, W, trans2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[1;32m---> 54\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape3f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m best, which \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mmaxout(Z)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackprop\u001b[39m(d_best: OutT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InT:\n",
      "File \u001b[1;32mc:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\thinc\\backends\\ops.py:635\u001b[0m, in \u001b[0;36mOps.reshape3f\u001b[1;34m(self, array, d0, d1, d2)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreshape2f\u001b[39m(\u001b[38;5;28mself\u001b[39m, array: FloatsXd, d0: \u001b[38;5;28mint\u001b[39m, d1: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Floats2d:\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Floats2d, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape(array, (d0, d1)))\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreshape3f\u001b[39m(\u001b[38;5;28mself\u001b[39m, array: FloatsXd, d0: \u001b[38;5;28mint\u001b[39m, d1: \u001b[38;5;28mint\u001b[39m, d2: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Floats3d:\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Floats3d, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape(array, (d0, d1, d2)))\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreshape4f\u001b[39m(\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28mself\u001b[39m, array: FloatsXd, d0: \u001b[38;5;28mint\u001b[39m, d1: \u001b[38;5;28mint\u001b[39m, d2: \u001b[38;5;28mint\u001b[39m, d3: \u001b[38;5;28mint\u001b[39m\n\u001b[0;32m    640\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Floats4d:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from collections import defaultdict,Counter\n",
    "import spacy\n",
    "import io\n",
    "\n",
    "de_tokenizer = spacy.load(\"de_core_news_sm\")\n",
    "en_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text,tokenizer):\n",
    "    tokens = tokenizer(text)\n",
    "    return [token.text.lower for token in tokens if not token.is_space]\n",
    "\n",
    "def build_vocab(file_name,tokenizer,min_freq=1,special_tokens=[\"<bos>\",\"<unk>\",\"<pad>\",\"<eos>\"]):\n",
    "    counter = Counter()\n",
    "    with io.open(file_name,encoding=\"utf-8\") as f:\n",
    "        for string_ in f:\n",
    "            tokens = tokenize(string_,tokenizer)\n",
    "            counter.update(tokens)\n",
    "            \n",
    "    tokens = [token for token,freq in counter.items() if freq >= min_freq]\n",
    "    vocab = {token:idx for idx,token in enumerate(tokens+special_tokens)}\n",
    "    unk_idx = vocab[\"<unk>\"]\n",
    "    vocab = defaultdict(lambda:unk_idx,vocab)\n",
    "    \n",
    "    return vocab\n",
    "    \n",
    "de_vocab = build_vocab(train_paths[0],tokenizer=de_tokenizer)\n",
    "en_vocab = build_vocab(train_paths[1],en_tokenizer)\n",
    "\n",
    "ModelArgs.en_vocab_size = len(en_vocab)+1\n",
    "ModelArgs.de_vocab_size = len(de_vocab)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(filenames):\n",
    "    raw_de_iter = iter(io.open(filenames[0],encoding=\"utf-8\"))\n",
    "    raw_en_iter = iter(io.open(filenames[1],encoding=\"utf-8\"))\n",
    "    data = []\n",
    "    for raw_de,raw_en in zip(raw_de_iter,raw_en_iter):\n",
    "        de_tensor = torch.tensor([de_vocab[token] for token in tokenize(raw_de,de_tokenizer)])\n",
    "        en_tensor = torch.tensor([en_vocab[token] for token in tokenize(raw_en,en_tokenizer)])\n",
    "        \n",
    "        en_tensor = torch.cat([torch.tensor([en_vocab[\"<bos>\"]]),en_tensor,torch.tensor([en_vocab[\"<eos>\"]])])\n",
    "        \n",
    "        de_tensor = torch.flip(de_tensor,dims=[0])\n",
    "        \n",
    "        data.append((de_tensor,en_tensor))\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_data = data_process(train_paths)\n",
    "val_data = data_process(val_paths)\n",
    "test_data = data_process(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_dataset = TranslationDataset(train_data)\n",
    "val_dataset = TranslationDataset(val_data)\n",
    "test_dataset = TranslationDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c7d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch,seq_len=ModelArgs.seq_len):\n",
    "    de_batch,en_batch = zip(*batch)\n",
    "    \n",
    "    def pad_or_truncate(sequence,pad_value):\n",
    "        if len(sequence) >= seq_len:\n",
    "            return sequence[:seq_len]\n",
    "        else:\n",
    "            pad_len = seq_len - len(sequence)\n",
    "            padding = torch.full([pad_len],fill_value=pad_value,dtype=sequence.dtype)\n",
    "            # print(f\"sequence : {sequence.shape}\")\n",
    "            # print(f\"padding : {padding.shape}\")\n",
    "            return torch.cat([sequence,padding])\n",
    "    de_batch = [pad_or_truncate(sample,pad_value=de_vocab[\"<pad>\"]) for sample in de_batch]\n",
    "    en_batch = [pad_or_truncate(sample,pad_value=en_vocab[\"<pad>\"]) for sample in en_batch]\n",
    "    \n",
    "    de_batch = torch.stack(de_batch)\n",
    "    en_batch = torch.stack(en_batch)\n",
    "    \n",
    "    return de_batch,en_batch\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=ModelArgs.batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=collate_fn,\n",
    "                              drop_last=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=ModelArgs.batch_size,\n",
    "                            shuffle=True,\n",
    "                            collate_fn=collate_fn,\n",
    "                            drop_last=True)\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=ModelArgs.batch_size,\n",
    "                             shuffle=True,\n",
    "                             collate_fn=collate_fn,\n",
    "                             drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252d7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 32]), torch.Size([32, 32]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(train_dataloader))\n",
    "sample_de = sample_batch[0]\n",
    "sample_en = sample_batch[1]\n",
    "\n",
    "sample_de.shape,sample_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06336908",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForgetGate(nn.Module):\n",
    "    def __init__(self,h_t_size,embedding_dim):\n",
    "        super().__init__()\n",
    "        self.sigma_nn = nn.Sequential(\n",
    "            nn.Linear(in_features=h_t_size+embedding_dim,out_features=h_t_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,X_t,h_t):\n",
    "        # print(f\"h_t : {h_t.shape}\")\n",
    "        # print(f\"X_t : {X_t.shape}\")\n",
    "        combined = torch.cat([h_t,X_t],dim=1)\n",
    "        # print(f\"combined : {combined.shape} \\n required : {self.sigma_nn}\")\n",
    "        f_t = self.sigma_nn(combined)\n",
    "        \n",
    "        return f_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f165648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputGate(nn.Module):\n",
    "    def __init__(self,h_t_size,embedding_dim):\n",
    "        super().__init__()\n",
    "        self.sigma_nn = nn.Sequential(\n",
    "            nn.Linear(in_features=h_t_size+embedding_dim,out_features=h_t_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.tanh_nn = nn.Sequential(\n",
    "            nn.Linear(in_features=h_t_size+embedding_dim,out_features=h_t_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self,X_t,h_t):\n",
    "        combined = torch.cat([h_t,X_t],dim=1)\n",
    "        \n",
    "        i_t = self.sigma_nn(combined)\n",
    "        c_t_dash = self.tanh_nn(combined)\n",
    "        \n",
    "        candidate_hidden_state = i_t * c_t_dash\n",
    "        return candidate_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bb582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputGate(nn.Module):\n",
    "    def __init__(self,h_t_size,embedding_dim):\n",
    "        super().__init__()\n",
    "        self.sigma_nn = nn.Sequential(\n",
    "            nn.Linear(in_features=h_t_size+embedding_dim,out_features=h_t_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,X_t,h_t):\n",
    "        combined = torch.cat([h_t,X_t],dim=1)\n",
    "        o_t = self.sigma_nn(combined)\n",
    "        return o_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d01331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self,h_t_size,embedding_dim):\n",
    "        super().__init__()\n",
    "        self.forget_gate = ForgetGate(h_t_size=h_t_size,embedding_dim=embedding_dim)\n",
    "        self.input_gate = InputGate(h_t_size=h_t_size,embedding_dim=embedding_dim)\n",
    "        self.output_gate = OutputGate(h_t_size=h_t_size,embedding_dim=embedding_dim)\n",
    "        \n",
    "    def forward(self,X_t,h_t,c_t):\n",
    "        f_t = self.forget_gate(X_t,h_t)\n",
    "        c_t = c_t * f_t\n",
    "        candidate_hidden_state = self.input_gate(X_t,h_t)\n",
    "        c_t = c_t + candidate_hidden_state\n",
    "        o_t = self.output_gate(X_t,h_t)\n",
    "        h_t_new = torch.tanh(c_t)*o_t\n",
    "        return h_t_new,c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e32f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self,h_t_size,embedding_dim):\n",
    "        super().__init__()\n",
    "        self.lstm_cell = LSTMCell(h_t_size=h_t_size,embedding_dim=embedding_dim)\n",
    "    def forward(self,X_t,h_t=None,c_t=None):\n",
    "        if h_t is None:\n",
    "            h_t = torch.zeros(size=[ModelArgs.batch_size,ModelArgs.h_t_size],device=ModelArgs.device)\n",
    "        if c_t is None:\n",
    "            c_t = torch.zeros(size=[ModelArgs.batch_size,ModelArgs.c_t_size],device=ModelArgs.device)\n",
    "        \n",
    "        h_t,c_t = self.lstm_cell(X_t,h_t,c_t) # for a single time a step\n",
    "        \n",
    "        return h_t,c_t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5173fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTable(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_dim)\n",
    "    def forward(self,X):\n",
    "        return self.embedding_layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,h_t_size,embedding_dim,no_of_layers,vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = EmbeddingTable(vocab_size=vocab_size,embedding_dim=embedding_dim)\n",
    "        self.encoder = nn.ModuleList([LSTMModel(h_t_size=h_t_size,embedding_dim=embedding_dim)])\n",
    "        for i in range(no_of_layers-1):\n",
    "            self.encoder.append(LSTMModel(h_t_size=h_t_size,embedding_dim=h_t_size)) # the input to the upper layers is h_t\n",
    "    \n",
    "    def forward(self,X):\n",
    "        outputs = []\n",
    "        for timestep in range(ModelArgs.seq_len):\n",
    "            X_t = X[:,timestep]\n",
    "            e_i = self.embedding_layer(X_t)\n",
    "            for layer in range(len(self.encoder)):\n",
    "                if timestep==0:\n",
    "                    h_t,c_t = self.encoder[layer](e_i)\n",
    "                else:\n",
    "                    h_t,c_t = self.encoder[layer](e_i,h_t,c_t)\n",
    "                e_i = h_t\n",
    "            outputs.append(h_t)\n",
    "        return torch.stack(outputs) , h_t, c_t# src_len * [batch_size,h_t_size] -> [batch_size,src_len,h_t_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoungAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,encoder_outputs,decoder_output):\n",
    "        # encoder_outputs -> [batch_size,src_len,h_t_size]\n",
    "        # decoder_output -> [batch_size,h_t_size]\n",
    "        decoder_output = decoder_output.unsqueeze(1) # [batch_size,1,h_t_size]\n",
    "        # print(f\"decoder_output : {decoder_output.shape}\")\n",
    "        # print(f\"encoder_outputs : {encoder_outputs.shape}\")\n",
    "        scores = torch.bmm(decoder_output,encoder_outputs.transpose(1,2)) # [batch_size,1,src_len]\n",
    "        attn_weights = torch.softmax(scores,dim=-1) # [batch_size,1,src_len]\n",
    "        \n",
    "        context = torch.bmm(attn_weights,encoder_outputs) # [batch_size,1,h_t_size]\n",
    "        return context.squeeze(),attn_weights.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d30fe",
   "metadata": {},
   "source": [
    "h_t_tilde = tanh(Wc[ct:ht])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,h_t_size,embedding_dim,no_of_layers,vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = EmbeddingTable(vocab_size=vocab_size,embedding_dim=embedding_dim)\n",
    "        self.decoder = nn.ModuleList([LSTMModel(h_t_size=h_t_size,embedding_dim=embedding_dim+h_t_size)]) # embedding_dim+h_t_size -> for decoder\n",
    "        for i in range(len(self.decoder)):\n",
    "            self.decoder.append(LSTMModel(h_t_size=h_t_size,embedding_dim=h_t_size))\n",
    "        self.attention = LoungAttention()\n",
    "        self.concat_context = nn.Linear(in_features=h_t_size+h_t_size,out_features=h_t_size)\n",
    "        self.classification_head = nn.Linear(in_features=h_t_size,out_features=vocab_size)\n",
    "    \n",
    "    def forward(self,encoder_outputs,h_t,c_t,X=None):\n",
    "        outputs = []\n",
    "        h_t_prev,c_t_prev = h_t,c_t\n",
    "        h_t_tilde_prev = torch.zeros(size=[ModelArgs.batch_size,ModelArgs.seq_len],device=ModelArgs.device)\n",
    "        for timestep in range(ModelArgs.seq_len):\n",
    "            if X is not None:\n",
    "                X_t = X[:,timestep]\n",
    "            else:\n",
    "                if timestep == 0:\n",
    "                    X_t = torch.full(size=[ModelArgs.batch_size],fill_value=en_vocab[\"<bos>\"],device=ModelArgs.device)\n",
    "                else:\n",
    "                    preds = torch.softmax(logits,dim=-1)\n",
    "                    X_t = torch.argmax(preds,dim=1)\n",
    "            e_i = self.embedding_layer(X_t)\n",
    "            input_t = torch.cat([e_i,h_t_tilde_prev],dim=1) # [batch_size,embed_dim+h_t_size]\n",
    "            for layer in range(len(self.decoder)):\n",
    "                h_t_prev,c_t_prev = self.decoder[layer](input_t,h_t_prev,c_t_prev)\n",
    "                input_t = h_t_prev\n",
    "            context,attn_weights = self.attention(encoder_outputs,h_t_prev) # h_t_prev is the current h_t generated by the decoder which is used by the Loung Attention to compute c_t (context vector)\n",
    "            concat = torch.cat([h_t_prev,context],dim=1) # h_t_prev is the current h_t generated by the decoder\n",
    "            h_t_tilde = torch.tanh(self.concat_context(concat))\n",
    "            logits = self.classification_head(h_t_tilde)\n",
    "            outputs.append(logits)\n",
    "            h_t_tilde_prev = h_t_tilde # input feeding approach\n",
    "        \n",
    "        return torch.stack(outputs) # [batch_size , seq_len ,vocab_size ]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9554d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,h_t_size,embedding_dim,src_vocab_size,dest_vocab_size,no_of_layers):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(h_t_size=h_t_size,embedding_dim=embedding_dim,no_of_layers=no_of_layers,vocab_size=src_vocab_size)\n",
    "        self.decoder = Decoder(h_t_size=h_t_size,embedding_dim=embedding_dim,no_of_layers=no_of_layers,vocab_size=dest_vocab_size)\n",
    "        \n",
    "    def forward(self,X,y=None):\n",
    "        encoder_outputs,h_t,c_t = self.encoder(X)\n",
    "        logits = self.decoder(encoder_outputs,h_t,c_t,y)\n",
    "        \n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30143184",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelArgs.embedding_dim = 16\n",
    "ModelArgs.no_of_layers = 4\n",
    "model = Seq2Seq(h_t_size=ModelArgs.h_t_size,embedding_dim=ModelArgs.embedding_dim,src_vocab_size=ModelArgs.de_vocab_size,dest_vocab_size=ModelArgs.en_vocab_size,no_of_layers=ModelArgs.no_of_layers)\n",
    "model = model.to(ModelArgs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59892c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 297118])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(sample_de.to(ModelArgs.device))\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd4dae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 297118])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(sample_de.to(ModelArgs.device),sample_en.to(ModelArgs.device))\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5370f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,model_name,criterion,optimizer,train_dataloader,val_dataloader,epochs,min_val_loss,device,writer,lr_scheduler):\n",
    "    try:\n",
    "        train_global_step,val_global_step = 1,1\n",
    "        metrics = {\"train_loss\":[],\"val_loss\":[],\"train_acc\":[],\"val_acc\":[]}\n",
    "        from tqdm import tqdm\n",
    "        best_val_loss = float(\"inf\")\n",
    "        model = model.to(device)\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_loss,correct,total = 0.0,0,0\n",
    "            train_progress = tqdm(train_dataloader,desc=\"Training\")\n",
    "            for idx,(de_batch,en_batch) in enumerate(train_progress):\n",
    "                de_batch = de_batch.to(device)\n",
    "                en_batch = en_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                all_logits = model(de_batch,en_batch)\n",
    "                # print(all_logits.shape)\n",
    "                all_logits = all_logits.view(-1,ModelArgs.en_vocab_size)\n",
    "                en_batch = en_batch.view(-1)\n",
    "                \n",
    "                loss = criterion(all_logits,en_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "                \n",
    "                pred_probs = torch.softmax(all_logits,dim=-1)\n",
    "                preds = torch.argmax(pred_probs,dim=1)\n",
    "                train_loss += loss.item()\n",
    "                correct += (preds == en_batch).sum()\n",
    "                total += en_batch.shape[0]\n",
    "                \n",
    "                train_progress.set_postfix({\"loss\":f\"{loss.item():.2f}\"})\n",
    "                \n",
    "                lr_scheduler.step(train_loss/(idx+1))\n",
    "                \n",
    "                metrics[\"train_loss\"].append(train_loss/(idx+1))\n",
    "                metrics[\"train_acc\"].append(correct/total)\n",
    "                \n",
    "                writer.add_scalar(\"loss/train_iter\",train_loss/(idx+1),train_global_step)\n",
    "                writer.add_scalar(\"accuracy/train_iter\",correct/total,train_global_step)\n",
    "                train_global_step += 1\n",
    "                \n",
    "                \n",
    "            train_loss /= len(train_dataloader)\n",
    "            train_acc = correct/total\n",
    "        \n",
    "            with torch.inference_mode():\n",
    "                model.eval()\n",
    "                val_loss,correct,total = 0.0,0,0\n",
    "                val_progress = tqdm(val_dataloader,desc=\"Evaluation\")\n",
    "                for idx,(de_batch,en_batch) in enumerate(val_progress):\n",
    "                    de_batch = de_batch.to(device)\n",
    "                    en_batch = en_batch.to(device)\n",
    "                    \n",
    "                    all_logits = model(de_batch,en_batch)\n",
    "                    all_logits = all_logits.view(-1,ModelArgs.en_vocab_size)\n",
    "                    en_batch = en_batch.view(-1)\n",
    "                    \n",
    "                    loss = criterion(all_logits,en_batch)\n",
    "                    \n",
    "                    pred_probs = torch.softmax(all_logits,dim=-1)\n",
    "                    preds = torch.argmax(pred_probs,dim=1)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    correct += (preds == en_batch).sum()\n",
    "                    total += en_batch.shape[0]\n",
    "                    \n",
    "                    val_progress.set_postfix({\"loss\":f\"{loss.item():.2f}\"})\n",
    "                    \n",
    "                    lr_scheduler.step(val_loss/(idx+1))\n",
    "                    \n",
    "                    metrics[\"val_loss\"].append(val_loss/(idx+1))\n",
    "                    metrics[\"val_acc\"].append(correct/total)\n",
    "                    \n",
    "                    writer.add_scalar(\"loss/val_iter\",val_loss/(idx+1),val_global_step)\n",
    "                    writer.add_scalar(\"accuracy/val_iter\",correct/total,val_global_step)\n",
    "                    val_global_step += 1\n",
    "                    \n",
    "                    \n",
    "                val_loss /= len(val_dataloader)\n",
    "                val_acc = correct/total\n",
    "                \n",
    "            print(f\"Epoch : {epoch}/{epochs} \\n Train Loss : {train_loss:.4f} , Train Acc : {train_acc:.4f} \\n Val Loss : {val_loss:.4f} Val Acc : {val_acc:.4f}\")\n",
    "            \n",
    "            # metrics[\"train_loss\"].append(train_loss)\n",
    "            # metrics[\"val_loss\"].append(val_loss)\n",
    "            # metrics[\"train_acc\"].append(train_acc)\n",
    "            # metrics[\"val_acc\"].append(val_acc)\n",
    "            \n",
    "            writer.add_scalar(\"loss/train_epoch\",train_loss,epoch)\n",
    "            writer.add_scalar(\"loss/val_epoch\",val_loss,epoch)\n",
    "            writer.add_scalar(\"accuracy/train_epoch\",train_acc,epoch)\n",
    "            writer.add_scalar(\"accuracy/val_epoch\",val_acc,epoch)\n",
    "                    \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(),model_name)\n",
    "            if val_loss < min_val_loss:\n",
    "                print(\"Model trained successfully....\")\n",
    "                break\n",
    "        return metrics\n",
    "    except KeyboardInterrupt:\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 906/906 [52:02<00:00,  3.45s/it, loss=3.74] \n",
      "Evaluation: 100%|| 31/31 [00:22<00:00,  1.35it/s, loss=3.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0/5000 \n",
      " Train Loss : 7.5311 , Train Acc : 0.4885 \n",
      " Val Loss : 3.7572 Val Acc : 0.5227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 906/906 [52:20<00:00,  3.47s/it, loss=1.44]\n",
      "Evaluation: 100%|| 31/31 [00:59<00:00,  1.91s/it, loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/5000 \n",
      " Train Loss : 2.2047 , Train Acc : 0.5281 \n",
      " Val Loss : 1.5142 Val Acc : 0.5237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 906/906 [53:59<00:00,  3.58s/it, loss=1.30] \n",
      "Evaluation: 100%|| 31/31 [00:23<00:00,  1.33it/s, loss=1.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2/5000 \n",
      " Train Loss : 1.3770 , Train Acc : 0.5281 \n",
      " Val Loss : 1.3076 Val Acc : 0.5238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|         | 100/906 [17:55<2:24:25, 10.75s/it, loss=1.32] \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=ModelArgs.max_lr)\n",
    "writer = SummaryWriter(\"runs/loung_attention\")\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode=\"min\",factor=0.1,patience=3,verbose=True)\n",
    "metrics = train(model=model,\n",
    "                model_name=\"loung_attention.pth\",\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                epochs=ModelArgs.epochs,\n",
    "                min_val_loss=1e-3,\n",
    "                device=ModelArgs.device,\n",
    "                writer=writer,\n",
    "                lr_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73907104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62222836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janar\\anaconda3\\envs\\stable\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Training:  54%|    | 488/906 [30:47<26:22,  3.79s/it, loss=1.31] \n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode=\"min\",factor=0.1,patience=3,verbose=True)\n",
    "\n",
    "model.load_state_dict(torch.load(\"loung_attention.pth\",weights_only=True))\n",
    "\n",
    "metrics = train(model=model,\n",
    "                model_name=\"loung_attention.pth\",\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                epochs=ModelArgs.epochs,\n",
    "                min_val_loss=1e-3,\n",
    "                device=ModelArgs.device,\n",
    "                writer=writer,\n",
    "                lr_scheduler=lr_scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf7a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59b529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metrics[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd900f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stable)",
   "language": "python",
   "name": "stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
